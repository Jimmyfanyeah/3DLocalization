{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lingjia/anaconda3/envs/decode_dev/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from cmath import inf\n",
    "import datetime\n",
    "import os\n",
    "import shutil\n",
    "import socket\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy\n",
    "import time\n",
    "import torch\n",
    "\n",
    "import decode.evaluation\n",
    "import decode.neuralfitter\n",
    "import decode.neuralfitter.coord_transform\n",
    "import decode.neuralfitter.utils\n",
    "import decode.simulation\n",
    "import decode.utils\n",
    "# from decode.neuralfitter.train.random_simulation import setup_random_simulation\n",
    "from decode.neuralfitter.utils import log_train_val_progress\n",
    "from decode.utils.checkpoint import CheckPoint\n",
    "torch.set_printoptions(precision=4,sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Training Args')\n",
    "\n",
    "    parser.add_argument('-i', '--device', default=None, \n",
    "                        help='Specify the device string (cpu, cuda, cuda:0) and overwrite param.',\n",
    "                        type=str)\n",
    "\n",
    "    parser.add_argument('-p', '--param_file', default=None,\n",
    "                        help='Specify your parameter file (.yml or .json).', type=str)\n",
    "\n",
    "    parser.add_argument('-w', '--num_worker_override',default=None,\n",
    "                        help='Override the number of workers for the dataloaders.',\n",
    "                        type=int)\n",
    "\n",
    "    parser.add_argument('-n', '--no_log', default=False, action='store_true',\n",
    "                        help='Set no log if you do not want to log the current run.')\n",
    "\n",
    "    parser.add_argument('-c', '--log_comment', default=None,\n",
    "                        help='Add a log_comment to the run.')\n",
    "\n",
    "    parser.add_argument('-d', '--data_path_override', default=None,\n",
    "                        help='Specify your path to data', type=str)\n",
    "\n",
    "    parser.add_argument('-is', '--img_size_override', default=None,\n",
    "                        help='Override img size', type=int)\n",
    "\n",
    "    # args = parser.parse_args()\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_trainer(logger, model_out, ckpt_path, device, param):\n",
    "    \"\"\"Set model, optimiser, loss and schedulers\"\"\"\n",
    "    models_available = {\n",
    "        'SigmaMUNet': decode.neuralfitter.models.SigmaMUNet,\n",
    "        'DoubleMUnet': decode.neuralfitter.models.model_param.DoubleMUnet,\n",
    "        'SimpleSMLMNet': decode.neuralfitter.models.model_param.SimpleSMLMNet,\n",
    "    }\n",
    "\n",
    "    model = models_available[param.HyperParameter.architecture]\n",
    "    model = model.parse(param)\n",
    "\n",
    "    model_ls = decode.utils.model_io.LoadSaveModel(model, output_file=model_out)\n",
    "\n",
    "    model = model_ls.load_init()\n",
    "    model = model.to(torch.device(device))\n",
    "\n",
    "    # Small collection of optimisers\n",
    "    optimizer_available = {\n",
    "        'Adam': torch.optim.Adam,\n",
    "        'AdamW': torch.optim.AdamW\n",
    "    }\n",
    "\n",
    "    optimizer = optimizer_available[param.HyperParameter.optimizer]\n",
    "    optimizer = optimizer(model.parameters(), **param.HyperParameter.opt_param)\n",
    "\n",
    "    \"\"\"Loss function.\"\"\"\n",
    "    criterion = decode.neuralfitter.loss.GaussianMMLoss(\n",
    "        xextent=param.Simulation.psf_extent[0],\n",
    "        yextent=param.Simulation.psf_extent[1],\n",
    "        img_shape=param.Simulation.img_size,\n",
    "        device=device,\n",
    "        chweight_stat=param.HyperParameter.chweight_stat)\n",
    "\n",
    "    \"\"\"Learning Rate and Simulation Scheduling\"\"\"\n",
    "    lr_scheduler_available = {\n",
    "        'ReduceLROnPlateau': torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "        'StepLR': torch.optim.lr_scheduler.StepLR\n",
    "    }\n",
    "    lr_scheduler = lr_scheduler_available[param.HyperParameter.learning_rate_scheduler]\n",
    "    lr_scheduler = lr_scheduler(optimizer, **param.HyperParameter.learning_rate_scheduler_param)\n",
    "\n",
    "    \"\"\"Checkpointing\"\"\"\n",
    "    checkpoint = CheckPoint(path=ckpt_path)\n",
    "\n",
    "    \"\"\"Setup gradient modification\"\"\"\n",
    "    grad_mod = param.HyperParameter.grad_mod\n",
    "\n",
    "    \"\"\"Log the model (Graph) \"\"\"\n",
    "    try:\n",
    "        dummy = torch.rand((2, param.HyperParameter.channels_in,\n",
    "                            *param.Simulation.img_size), requires_grad=False).to(torch.device(device))\n",
    "        logger.add_graph(model, dummy)\n",
    "\n",
    "    except:\n",
    "        print(\"Did not log graph.\")\n",
    "        # raise RuntimeError(\"Your dummy input is wrong. Please update it.\")\n",
    "\n",
    "    \"\"\"Setup Target generator consisting possibly multiple steps in a transformation sequence.\"\"\"\n",
    "    tar_proc = decode.neuralfitter.utils.processing.TransformSequence(\n",
    "        [\n",
    "            # param_tar --> phot/max, z/z_max, bg/bg_max\n",
    "            decode.neuralfitter.scale_transform.ParameterListRescale(\n",
    "                phot_max=param.Scaling.phot_max,\n",
    "                z_max=param.Scaling.z_max,\n",
    "                bg_max=param.Scaling.bg_max)\n",
    "        ])\n",
    "\n",
    "    # train_IDs = numpy.arange(1,9001,1).tolist()\n",
    "    # val_IDs = numpy.arange(9001,10001,1).tolist()\n",
    "    train_IDs = numpy.arange(0,9000,1).tolist()\n",
    "    val_IDs = numpy.arange(9000,10000,1).tolist()\n",
    "\n",
    "    train_ds = decode.neuralfitter.dataset.rPSFDataset(root_dir=param.InOut.data_path,\n",
    "                                                       list_IDs=train_IDs, label_path=None, \n",
    "                                                       n_max=param.HyperParameter.max_number_targets,\n",
    "                                                       tar_proc=tar_proc,\n",
    "                                                       img_shape=param.Simulation.img_size)\n",
    "\n",
    "    test_ds = decode.neuralfitter.dataset.rPSFDataset(root_dir=param.InOut.data_path,\n",
    "                                                       list_IDs=val_IDs, label_path=None, \n",
    "                                                       n_max=param.HyperParameter.max_number_targets,\n",
    "                                                       tar_proc=tar_proc,\n",
    "                                                       img_shape=param.Simulation.img_size)\n",
    "\n",
    "    # print(test_ds.label_gen())\n",
    "\n",
    "    \"\"\"Set up post processor\"\"\"\n",
    "    if param.PostProcessing is None:\n",
    "        post_processor = decode.neuralfitter.post_processing.NoPostProcessing(xy_unit='px',\n",
    "                                                                              px_size=param.Camera.px_size)\n",
    "\n",
    "    elif param.PostProcessing == 'LookUp':\n",
    "        post_processor = decode.neuralfitter.utils.processing.TransformSequence([\n",
    "\n",
    "            decode.neuralfitter.scale_transform.InverseParamListRescale(\n",
    "                phot_max=param.Scaling.phot_max,\n",
    "                z_max=param.Scaling.z_max,\n",
    "                bg_max=param.Scaling.bg_max),\n",
    "\n",
    "            decode.neuralfitter.coord_transform.Offset2Coordinate.parse(param),\n",
    "\n",
    "            decode.neuralfitter.post_processing.LookUpPostProcessing(\n",
    "                raw_th=param.PostProcessingParam.raw_th,\n",
    "                pphotxyzbg_mapping=[0, 1, 2, 3, 4, -1],\n",
    "                xy_unit='px',\n",
    "                px_size=param.Camera.px_size)\n",
    "        ])\n",
    "\n",
    "    elif param.PostProcessing in ('SpatialIntegration', 'NMS'):  # NMS as legacy support\n",
    "        post_processor = decode.neuralfitter.utils.processing.TransformSequence([\n",
    "            # out_tar --> out_tar: photo*photo_max, z*z_max, bg*bg_max\n",
    "            decode.neuralfitter.scale_transform.InverseParamListRescale(\n",
    "                phot_max=param.Scaling.phot_max,\n",
    "                z_max=param.Scaling.z_max,\n",
    "                bg_max=param.Scaling.bg_max),\n",
    "            # offset --> coordinate e.g., 0.2 --> 10.2 \n",
    "            decode.neuralfitter.coord_transform.Offset2Coordinate.parse(param),\n",
    "\n",
    "            decode.neuralfitter.post_processing.SpatialIntegration(\n",
    "                raw_th=param.PostProcessingParam.raw_th, # 0.5\n",
    "                xy_unit='px')\n",
    "        ])\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    \"\"\"Evaluation Specification\"\"\"\n",
    "    matcher = decode.evaluation.match_emittersets.GreedyHungarianMatching.parse(param)\n",
    "    # matcher = None\n",
    "\n",
    "    return train_ds, test_ds, model, model_ls, optimizer, criterion, lr_scheduler, grad_mod, post_processor, matcher, checkpoint\n",
    "\n",
    "\n",
    "def setup_dataloader(param, train_ds, test_ds=None):\n",
    "    \"\"\"Set up dataloader\"\"\"\n",
    "\n",
    "    train_dl = torch.utils.data.DataLoader(\n",
    "        dataset=train_ds,\n",
    "        batch_size=param.HyperParameter.batch_size,\n",
    "        drop_last=True,\n",
    "        shuffle=True,\n",
    "        num_workers=param.Hardware.num_worker_train,\n",
    "        pin_memory=True,\n",
    "        collate_fn=decode.neuralfitter.utils.dataloader_customs.smlm_collate)\n",
    "\n",
    "    if test_ds is not None:\n",
    "\n",
    "        test_dl = torch.utils.data.DataLoader(\n",
    "            dataset=test_ds,\n",
    "            batch_size=param.HyperParameter.batch_size,\n",
    "            drop_last=False,\n",
    "            shuffle=False,\n",
    "            num_workers=param.Hardware.num_worker_train,\n",
    "            pin_memory=False,\n",
    "            collate_fn=decode.neuralfitter.utils.dataloader_customs.smlm_collate)\n",
    "    else:\n",
    "\n",
    "        test_dl = None\n",
    "\n",
    "    return train_dl, test_dl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_args()\n",
    "args.device='cuda:3'\n",
    "args.param_file='/home/lingjia/Documents/rPSF/NN/param_v2.yaml'\n",
    "args.data_path_override='/media/hdd/rPSF/data/plain/train/0620_uniformFlux'\n",
    "args.img_size_override=96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1 Train Time:6.5e+01 Loss:2.41e+02: 100%|█████████████████████████| 281/281 [01:04<00:00,  4.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# for i in range(first_epoch, param.HyperParameter.epochs):\n",
    "i=1\n",
    "logger.add_scalar('learning/learning_rate', optimizer.param_groups[0]['lr'], i)\n",
    "print(f'Epoch{i}')\n",
    "\n",
    "if i >= 1:\n",
    "    _ = decode.neuralfitter.train_val_impl.train(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        loss=criterion,\n",
    "        dataloader=dl_train,\n",
    "        grad_rescale=param.HyperParameter.moeller_gradient_rescale,\n",
    "        grad_mod=grad_mod,\n",
    "        epoch=i,\n",
    "        device=torch.device(device),\n",
    "        logger=logger\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from typing import Union\n",
    "from tqdm import tqdm\n",
    "from collections import namedtuple\n",
    "import sys\n",
    "\n",
    "from decode.neuralfitter.utils import log_train_val_progress\n",
    "from decode.evaluation.utils import MetricMeter\n",
    "from torch import distributions\n",
    "\n",
    "def ship_device(x, device: Union[str, torch.device]):\n",
    "    \"\"\"\n",
    "    Ships the input to a pytorch compatible device (e.g. CUDA)\n",
    "\n",
    "    Args:\n",
    "        x:\n",
    "        device:\n",
    "\n",
    "    Returns:\n",
    "        x\n",
    "\n",
    "    \"\"\"\n",
    "    if x is None:\n",
    "        return x\n",
    "\n",
    "    elif isinstance(x, torch.Tensor):\n",
    "        return x.to(device)\n",
    "\n",
    "    elif isinstance(x, (tuple, list)):\n",
    "        x = [ship_device(x_el, device) for x_el in x]  # a nice little recursion that worked at the first try\n",
    "        return x\n",
    "\n",
    "    elif device != 'cpu':\n",
    "        raise NotImplementedError(f\"Unsupported data type for shipping from host to CUDA device.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/lingjia/Documents/rPSF/NN/main_v2.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B144.214.74.91/home/lingjia/Documents/rPSF/NN/main_v2.ipynb#ch0000007vscode-remote?line=4'>5</a>\u001b[0m grad_rescale\u001b[39m=\u001b[39mparam\u001b[39m.\u001b[39mHyperParameter\u001b[39m.\u001b[39mmoeller_gradient_rescale\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B144.214.74.91/home/lingjia/Documents/rPSF/NN/main_v2.ipynb#ch0000007vscode-remote?line=5'>6</a>\u001b[0m grad_mod\u001b[39m=\u001b[39mgrad_mod\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B144.214.74.91/home/lingjia/Documents/rPSF/NN/main_v2.ipynb#ch0000007vscode-remote?line=6'>7</a>\u001b[0m epoch\u001b[39m=\u001b[39mi\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B144.214.74.91/home/lingjia/Documents/rPSF/NN/main_v2.ipynb#ch0000007vscode-remote?line=7'>8</a>\u001b[0m device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(device)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B144.214.74.91/home/lingjia/Documents/rPSF/NN/main_v2.ipynb#ch0000007vscode-remote?line=8'>9</a>\u001b[0m logger\u001b[39m=\u001b[39mlogger\n",
      "\u001b[0;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "model=model\n",
    "optimizer=optimizer\n",
    "loss=criterion\n",
    "dataloader=dl_train\n",
    "grad_rescale=param.HyperParameter.moeller_gradient_rescale\n",
    "grad_mod=grad_mod\n",
    "epoch=i\n",
    "device=torch.device(device)\n",
    "logger=logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                       | 0/281 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Actual Training\"\"\"\n",
    "\"\"\"Some Setup things\"\"\"\n",
    "model.train()\n",
    "tqdm_enum = tqdm(dataloader, total=len(dataloader), smoothing=0.,ncols=100)  # progress bar enumeration\n",
    "t0 = time.time()\n",
    "loss_epoch = MetricMeter()\n",
    "loss_gmm_epoch = MetricMeter()\n",
    "loss_bg_epoch = MetricMeter()\n",
    "\n",
    "# for batch_num, (x, y_tar, weight) in enumerate(tqdm_enum):  # model input (x), target (yt), weights (w)\n",
    "    # x = frames, y_tar =  Tuple(param_tar, mask_tar, bg), weight = None\n",
    "batch_num, (x, y_tar, weight) = next(enumerate(tqdm_enum))\n",
    "# print(y_tar[0][0,:5,:])\n",
    "# print(y_tar[1][0,:5])\n",
    "# print(y_tar[1][0,-5:])\n",
    "# print(y_tar[2][0,:1,:1])\n",
    "# print(type(weight))\n",
    "\n",
    "\"\"\"Monitor time to get the data\"\"\"\n",
    "t_data = time.time() - t0\n",
    "\n",
    "\"\"\"Ship the data to the correct device\"\"\"\n",
    "x, y_tar, weight = ship_device([x, y_tar, weight], device)\n",
    "\n",
    "\"\"\"Forward the data\"\"\"\n",
    "y_out = model(x)\n",
    "# print(y_out.shape) # [32, 10, 96, 96]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogProb: -1177.076416015625, GMM_log: 30.081775665283203\n",
      "tensor([ 1239.6512,     0.0309], device='cuda:2', grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss = decode.neuralfitter.loss.GaussianMMLoss(\n",
    "    xextent=param.Simulation.psf_extent[0],\n",
    "    yextent=param.Simulation.psf_extent[1],\n",
    "    img_shape=param.Simulation.img_size,\n",
    "    device=device,\n",
    "    chweight_stat=param.HyperParameter.chweight_stat)\n",
    "\n",
    "\"\"\"Reset the optimiser, compute the loss and backprop it\"\"\"\n",
    "loss_val = loss(y_out, y_tar, weight)\n",
    "print(loss_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0309, device='cuda:2', grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = y_out\n",
    "target = y_tar\n",
    "weight = weight\n",
    "\n",
    "# def forward(self, output: torch.Tensor, target: Tuple[torch.Tensor, torch.Tensor, torch.Tensor],\n",
    "#             weight: None) -> torch.Tensor:\n",
    "\n",
    "tar_param, tar_mask, tar_bg = target\n",
    "# p, pxyz_mu, pxyz_sig, bg = self._format_model_output(output)\n",
    "p = output[:, 0]\n",
    "pxyz_mu = output[:, 1:5]\n",
    "pxyz_sig = output[:, 5:-1]\n",
    "bg = output[:, -1]\n",
    "# print(torch.max(p))\n",
    "# print(pxyz_mu.shape)\n",
    "\n",
    "\"\"\"\" Background Loss \"\"\"\n",
    "_bg_loss = torch.nn.MSELoss(reduction='none')\n",
    "bg_loss = _bg_loss(bg, tar_bg).sum(-1).sum(-1)\n",
    "print(bg_loss[0]*2)\n",
    "# print(torch.max(bg_loss))\n",
    "# print(bg[0,0,:5])\n",
    "# print(tar_bg[0,0,:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = p\n",
    "pxyz_mu = pxyz_mu\n",
    "pxyz_sig = pxyz_sig\n",
    "pxyz_tar = tar_param\n",
    "mask = tar_mask\n",
    "\n",
    "batch_size = pxyz_mu.size(0) # 32\n",
    "log_prob = 0\n",
    "\n",
    "p_mean = p.sum(-1).sum(-1) # shape = [32]\n",
    "# print(p_mean)\n",
    "\n",
    "p_var = (p - p ** 2).sum(-1).sum(-1)  # var estimate of bernoulli\n",
    "# print(p_var)\n",
    "\n",
    "p_gauss = distributions.Normal(p_mean, torch.sqrt(p_var))\n",
    "# print(p_gauss.log_prob(mask.sum(-1)))\n",
    "# print(-((x - mu) ** 2) / (2 * sig**2) - math.log(sig) - math.log(math.sqrt(2 * math.pi)))\n",
    "\n",
    "# print(mask.sum(-1))\n",
    "# [39,  3,  1, 16,  3, 27, 12, 19,  4,  7,  5, 39, 27, 25, 28, 16, 19, 16,\n",
    "#          5, 37, 15, 23, 28,  2, 19, 19, 28,  7, 26,  3, 21,  8],\n",
    "#        device='cuda:2')\n",
    "\n",
    "log_prob = log_prob + p_gauss.log_prob(mask.sum(-1)) * mask.sum(-1)\n",
    "# print(log_prob)\n",
    "\n",
    "prob_normed = p / p.sum(-1).sum(-1).view(-1, 1, 1)\n",
    "# print(torch.max(prob_normed))\n",
    "# 0.1479"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-21.7802, device='cuda:2', grad_fn=<SelectBackward>)\n",
      "tensor(641.6058, device='cuda:2', grad_fn=<SelectBackward>)\n",
      "tensor(1239.6512, device='cuda:2', grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "img_shape=param.Simulation.img_size\n",
    "xextent=param.Simulation.psf_extent[0]\n",
    "yextent=param.Simulation.psf_extent[1]\n",
    "_bin_x, _bin_y, bin_ctr_x, bin_ctr_y = decode.generic.utils.frame_grid(img_shape, xextent, yextent)\n",
    "\n",
    "p = output[:, 0]\n",
    "pxyz_mu = output[:, 1:5]\n",
    "pxyz_sig = output[:, 5:-1]\n",
    "\n",
    "\"\"\"Hacky way to get all prob indices\"\"\"\n",
    "p_inds = tuple((p + 1).nonzero(as_tuple=False).transpose(1, 0))\n",
    "pxyz_mu = pxyz_mu[p_inds[0], :, p_inds[1], p_inds[2]]\n",
    "\n",
    "\"\"\"Convert px shifts to absolute coordinates\"\"\"\n",
    "pxyz_mu[:, 1] += bin_ctr_x[p_inds[1]].to(pxyz_mu.device)\n",
    "pxyz_mu[:, 2] += bin_ctr_y[p_inds[2]].to(pxyz_mu.device)\n",
    "\n",
    "\"\"\"Flatten img dimension --> N x (HxW) x 4\"\"\"\n",
    "pxyz_mu = pxyz_mu.reshape(batch_size, -1, 4)\n",
    "pxyz_sig = pxyz_sig[p_inds[0], :, p_inds[1], p_inds[2]].reshape(batch_size, -1, 4)\n",
    "\n",
    "\"\"\"Set up mixture family\"\"\"\n",
    "mix = distributions.Categorical(prob_normed[p_inds].reshape(batch_size, -1))\n",
    "comp = distributions.Independent(distributions.Normal(pxyz_mu, pxyz_sig), 1)\n",
    "gmm = distributions.mixture_same_family.MixtureSameFamily(mix, comp)\n",
    "# print(f'gmm:{gmm}')\n",
    "\n",
    "\"\"\"Calc log probs if there is anything there\"\"\"\n",
    "if mask.sum():\n",
    "    # print(f'pxyz_tar:{pxyz_tar.shape}')\n",
    "    gmm_log = gmm.log_prob(pxyz_tar.transpose(0, 1)).transpose(0, 1)\n",
    "    gmm_log = (gmm_log * mask).sum(-1)\n",
    "    # print(f\"LogProb: {log_prob.mean()}, GMM_log: {gmm_log.mean()}\")\n",
    "    # log_prob = log_prob + gmm_log\n",
    "    gmm_loss = gmm_log\n",
    "\n",
    "# log_prob = log_prob.reshape(batch_size, 1)  # need?\n",
    "\n",
    "gmm_loss = gmm_loss * (-1)\n",
    "log_prob = log_prob * (-1)\n",
    "print(gmm_loss[0])\n",
    "print(log_prob[0])\n",
    "print((log_prob[0]+gmm_loss[0])*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-115.3188)\n",
      "-115.31881669101394\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "x = 39\n",
    "mu = 130.7310\n",
    "sig = math.sqrt(37.3684)\n",
    "\n",
    "p_gauss = distributions.Normal(mu,sig)\n",
    "print(p_gauss.log_prob(x))\n",
    "print(-((x - mu) ** 2) / (2 * sig**2) - math.log(sig) - math.log(math.sqrt(2 * math.pi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if grad_rescale:  # rescale gradients so that they are in the same order for the last layer\n",
    "    weight, _, _ = model.rescale_last_layer_grad(loss_val, optimizer)\n",
    "    loss_val = loss_val * weight\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss_val.mean().backward()\n",
    "\n",
    "\"\"\"Gradient Modification\"\"\"\n",
    "if grad_mod:\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.03, norm_type=2)\n",
    "\n",
    "\"\"\"Update model parameters\"\"\"\n",
    "optimizer.step()\n",
    "\n",
    "\"\"\"Monitor overall time\"\"\"\n",
    "t_batch = time.time() - t0\n",
    "\n",
    "\"\"\"Logging\"\"\"\n",
    "loss_mean, loss_cmp = loss.log(loss_val)  # compute individual loss components\n",
    "loss_gmm = loss_cmp['gmm']\n",
    "loss_bg = loss_cmp['bg']\n",
    "del loss_val\n",
    "loss_epoch.update(loss_mean)\n",
    "loss_gmm_epoch.update(loss_gmm)\n",
    "loss_bg_epoch.update(loss_bg)\n",
    "tqdm_enum.set_description(f\"{epoch} Train Time:{t_batch:.2} Loss:{loss_mean:.3}\")\n",
    "\n",
    "# t0 = time.time()\n",
    "\n",
    "# log_train_val_progress.log_train(loss_p_batch=loss_epoch.vals, loss_mean=loss_epoch.mean, logger=logger, step=epoch)\n",
    "\n",
    "log_train_val_progress.log_train(loss_p_batch=loss_epoch.vals, loss_mean=loss_epoch.mean, logger=logger, step=epoch,loss_gmm_mean=loss_gmm_epoch.mean,loss_bg_mean=loss_bg_epoch.mean)\n",
    "\n",
    "return loss_epoch.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# val_loss=avg of loss for all batches\n",
    "# test_out=list of network_output: [\"loss\", \"x\", \"y_out\", \"y_tar\", \"weight\", \"em_tar\"]\n",
    "val_loss, test_out = decode.neuralfitter.train_val_impl.test(\n",
    "    model=model,\n",
    "    loss=criterion,\n",
    "    dataloader=dl_test,\n",
    "    epoch=i,\n",
    "    device=torch.device(device))\n",
    "# print(val_loss)\n",
    "\n",
    "if best_val_loss - val_loss >1e-4:\n",
    "    best_val_loss = val_loss\n",
    "    # model_ls.save(model, None, epoch_idx='best')\n",
    "\n",
    "t0 = time.time()\n",
    "if i%3 == 0:\n",
    "    \"\"\"Post-Process and Evaluate\"\"\"\n",
    "    log_train_val_progress.post_process_log_test(loss_cmp=test_out.loss,\n",
    "                                                loss_scalar=val_loss,\n",
    "                                                x=test_out.x, y_out=test_out.y_out,\n",
    "                                                y_tar=test_out.y_tar,\n",
    "                                                weight=test_out.weight,\n",
    "                                                em_tar=ds_test.emitter(),\n",
    "                                                px_border=-0.5, px_size=1.,\n",
    "                                                post_processor=post_processor,\n",
    "                                                matcher=matcher, logger=logger,\n",
    "                                                step=i)\n",
    "else:\n",
    "    log_train_val_progress.log_kpi_simplified(loss_scalar=val_loss,\n",
    "                                            loss_cmp=test_out.loss,\n",
    "                                            logger=logger,\n",
    "                                            step=i)\n",
    "\n",
    "t_log = time.time() - t0\n",
    "print(f'log time:{t_log}')\n",
    "\n",
    "if i >= 1:\n",
    "    if isinstance(lr_scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "        lr_scheduler.step(val_loss)\n",
    "    else:\n",
    "        lr_scheduler.step()\n",
    "\n",
    "# print(\"Training finished after reaching maximum number of epochs.\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "32d9234fb3a060fbd30877034f28c0fca724fa3d0d25a605ad46b1806c555f07"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('decode_dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
