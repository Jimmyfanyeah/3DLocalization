{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lingjia/anaconda3/envs/deepstorm3d/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# official modules\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from math import floor\n",
    "from shutil import copy2\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "# self-defined module\n",
    "from utils.helper import init_DDP, Logger, print_log, load_labels, build_model\n",
    "from utils.data import dataloader\n",
    "from utils.train_model import train_model\n",
    "from utils.test_model import test_model\n",
    "\n",
    "\n",
    "# packages for train_model.py\n",
    "# official modules\n",
    "import os\n",
    "import time\n",
    "from time import localtime, strftime\n",
    "from math import ceil\n",
    "import pickle5 as pickle\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.distributed as dist\n",
    "# self-defined modules\n",
    "from utils.loss import calculate_loss\n",
    "from utils.helper import print_log, print_metrics, print_time, print_metric_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='3d localization')\n",
    "# phase\n",
    "parser.add_argument('--train_or_test', type=str, default='other', help='train or test')\n",
    "parser.add_argument('--resume', action='store_true', default=False)\n",
    "parser.add_argument('--gpu_number', type=str, default=None, help='assign gpu')\n",
    "# data info\n",
    "parser.add_argument('--num_im', type=int, default=None, help='Number of samples used, train:val=9:1')\n",
    "parser.add_argument('--H', type=int, default=96, help='Height of image')\n",
    "parser.add_argument('--W', type=int, default=96, help='Width of image')\n",
    "parser.add_argument('--zmin', type=int, default=-20, help='min zeta')\n",
    "parser.add_argument('--zmax', type=int, default=20, help='max zeta')\n",
    "parser.add_argument('--clear_dist', type=int, default=1, help='safe margin for z axis')\n",
    "parser.add_argument('--D', type=int, default=250, help='num grid of zeta axis')\n",
    "parser.add_argument('--scaling_factor', type=int, default=800, help='entry value for existence of pts')\n",
    "parser.add_argument('--upsampling_factor', type=int, default=2, help='grid dim=[H,W]*upsampling_factor')\n",
    "# train info\n",
    "parser.add_argument('--model_use', type=str, default='LocNet')\n",
    "parser.add_argument('--postpro',  action='store_true', default=False, help='whether do post processing in dnn')\n",
    "parser.add_argument('--batch_size', type=int, default=1, help='when training on multi GPU, is the batch size on each GPU')\n",
    "parser.add_argument('--initial_learning_rate', type=float, default=None, help='initial learning rate for adam')\n",
    "parser.add_argument('--lr_decay_per_epoch', type=int, default=None, help='number of epochs learning rate decay')\n",
    "parser.add_argument('--lr_decay_factor', type=float, default=None, help='lr decay factor')\n",
    "parser.add_argument('--max_epoch', type=int,   default=None, help='number of training epoches')\n",
    "parser.add_argument('--save_epoch', type=int, default=None, help='save model per save_epoch')\n",
    "# test info\n",
    "parser.add_argument('--test_id_loc', type=str, default=None)\n",
    "# path\n",
    "parser.add_argument('--checkpoint_path', type=str,  default=None,  help='checkpoint to resume from')\n",
    "parser.add_argument('--data_path', type=str, default='/home/lingjia/Documents/microscope/Data/training_images_zrange20', help='path for train and val data')\n",
    "parser.add_argument('--save_path', type=str, default=None, help='path for save models and results')\n",
    "# output\n",
    "parser.add_argument('--name_time', type=str, default=None, help='string of running time')\n",
    "# for nonconvex loss\n",
    "parser.add_argument('--port', type=str, default=None, help='DDP master port')\n",
    "parser.add_argument('--weight', type=str, default=None, help='lambda CEL0')\n",
    "parser.add_argument('--extra_loss', type=str, default=None, help='indicate whether use cel0 for gaussian or nc for possion')\n",
    "# for extra losses\n",
    "parser.add_argument('--cel0_mu', type=float, default=None, help='mu in cel0 loss')\n",
    "parser.add_argument('--klnc_a', type=float, default=None, help='a for nonconvex loss in KLNC')\n",
    "\n",
    "# opt = parser.parse_args()\n",
    "opt,_=parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.H=96\n",
    "opt.W=96\n",
    "opt.zmin=-20\n",
    "opt.zmax=20\n",
    "opt.clear_dist=1\n",
    "opt.D=250\n",
    "opt.scaling_factor=800\n",
    "opt.upsampling_factor=2\n",
    "opt.model_use='LocNet'\n",
    "\n",
    "opt.initial_learning_rate=1e-3\n",
    "opt.lr_decay_per_epoch=7\n",
    "opt.lr_decay_factor=0.5\n",
    "opt.max_epoch=2\n",
    "opt.save_epoch=10\n",
    "opt.train_or_test='train'\n",
    "opt.port='123789'\n",
    "\n",
    "opt.ame_time=\"2099-12-12-12-12-12\"\n",
    "opt.num_im=10000\n",
    "opt.data_path='/media/hdd/lingjia/hdd_rpsf/nonconvex_loss/data/gaussian_10k_pt50L5'\n",
    "opt.save_path='/media/hdd/lingjia/hdd_rpsf/nonconvex_loss/temp'\n",
    "\n",
    "opt.weight='1_1'\n",
    "opt.extra_loss='mse3d_cel0'\n",
    "opt.cel0_mu=0.001\n",
    "\n",
    "opt.gpu_number = '0'\n",
    "\n",
    "opt.batch_size=24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 0\n",
    "world_size = 1\n",
    "\n",
    "opt.rank = rank\n",
    "opt.world_size = world_size\n",
    "\n",
    "init_DDP(opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO==>] setup_params:\n",
      "[INFO==>] Dataset: Train 9000 Val 1000\n",
      "[INFO==>] Number of parameters: 1594162\n"
     ]
    }
   ],
   "source": [
    "# calculate zoom ratio of z-axis\n",
    "opt.pixel_size_axial = (opt.zmax - opt.zmin + 1 + 2*opt.clear_dist) / opt.D\n",
    "\n",
    "# split dataset to train, validation 9:1\n",
    "train_IDs = np.arange(1,floor(opt.num_im*0.9)+1,1).tolist()\n",
    "val_IDs = np.arange(floor(opt.num_im*0.9)+1,opt.num_im+1).tolist()\n",
    "\n",
    "opt.partition = {'train': train_IDs, 'valid': val_IDs}\n",
    "opt.ntrain, opt.nval = len(train_IDs), len(val_IDs)\n",
    "\n",
    "# output folder name\n",
    "name_time = opt.name_time if opt.name_time else time.strftime('%Y-%m-%d-%H-%M-%S')\n",
    "save_name = name_time + '-lr'+str(opt.initial_learning_rate) + \\\n",
    "    '-bs'+str(opt.batch_size) + \\\n",
    "    '-D'+str(opt.D) + \\\n",
    "    '-Ep'+str(opt.max_epoch) + \\\n",
    "    '-nT'+str(opt.ntrain)\n",
    "if opt.extra_loss:\n",
    "    save_name = save_name + '-w' + str(opt.weight) + '-' + str(opt.extra_loss)\n",
    "if opt.cel0_mu:\n",
    "    save_name = save_name + '-' + str(opt.cel0_mu)\n",
    "if opt.klnc_a:\n",
    "    save_name = save_name + '-' + str(opt.klnc_a)\n",
    "save_name = save_name + '-' + str(opt.model_use)\n",
    "\n",
    "if opt.resume:\n",
    "    save_name = save_name + '-resume'\n",
    "if opt.postpro:\n",
    "    save_name = save_name + '-postpro'\n",
    "opt.save_path = os.path.join(opt.save_path,save_name)\n",
    "os.makedirs(opt.save_path, exist_ok=True)\n",
    "\n",
    "# log files\n",
    "if rank == 0:\n",
    "    log = open(os.path.join(opt.save_path, '{}_log.txt'.format(time.strftime('%H-%M-%S'))), 'w')\n",
    "    logger = Logger(os.path.join(opt.save_path, '{}_tensorboard'.format(time.strftime('%H-%M-%S'))))\n",
    "\n",
    "    print_log('[INFO==>] setup_params:',log)\n",
    "    # for key,value in opt._get_kwargs():\n",
    "        # if not key == 'partition':\n",
    "            # print_log('{}: {}'.format(key,value),log)\n",
    "    print_log(f'[INFO==>] Dataset: Train {len(train_IDs)} Val {len(val_IDs)}',log)\n",
    "\n",
    "\n",
    "device = torch.device('cuda')\n",
    "# what is this?\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "if opt.rank==0:\n",
    "    # save setup parameters in result folder as well\n",
    "    with open(os.path.join(opt.save_path,'setup_params.json'),'w') as handle:\n",
    "        json.dump(opt.__dict__, handle, indent=2)\n",
    "\n",
    "# Load labels and generate dataset\n",
    "labels = load_labels(os.path.join(opt.data_path,'label.txt'))\n",
    "\n",
    "# Parameters for dataloaders\n",
    "params_train = {'batch_size': opt.batch_size, 'shuffle': True,  'partition': opt.partition['train']}\n",
    "params_val = {'batch_size': opt.batch_size, 'shuffle': False, 'partition': opt.partition['valid']}\n",
    "\n",
    "training_generator = dataloader(opt.data_path, labels, params_train, opt, num_workers=0)\n",
    "validation_generator = dataloader(opt.data_path, labels, params_val, opt, num_workers=0)\n",
    "\n",
    "# model\n",
    "model = build_model(opt)\n",
    "model.to(device)\n",
    "model = DDP(model,find_unused_parameters=True,broadcast_buffers=False)\n",
    "\n",
    "optimizer = Adam(list(model.parameters()), lr=opt.initial_learning_rate)\n",
    "\n",
    "# opt.scheduler_type = 'ReduceLROnPlateau'\n",
    "# if opt.scheduler_type == 'StepLR':\n",
    "# scheduler = lr_scheduler.StepLR(optimizer, step_size=opt.lr_decay_per_epoch, gamma=opt.lr_decay_factor)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=opt.lr_decay_factor, patience=opt.lr_decay_per_epoch)\n",
    "\n",
    "# Print Model layers and number of parameters\n",
    "if opt.rank == 0:\n",
    "    # print_log(model, log)\n",
    "    print_log(\"[INFO==>] Number of parameters: {}\".format(sum(param.numel() for param in model.parameters())),log)\n",
    "\n",
    "\n",
    "# if resume_training, continue from a checkpoint\n",
    "if opt.resume:\n",
    "    print_log(\"[INFO==>] Resume...\",log)\n",
    "    checkpoint = torch.load(opt.checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    # optimizer.load_state_dict(checkpoint['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO==>] Loss types: ['loss', 'mse3d', 'cel0']\n",
      "[INFO==>] Start training from 0 to 2 rank 0\n",
      "\n",
      "---> Epoch 1/2 | 2022-10-16 20:30:02 | lr 0.001\n"
     ]
    }
   ],
   "source": [
    "learning_results = defaultdict(list)\n",
    "max_epoch = opt.max_epoch\n",
    "\n",
    "steps_train = ceil(opt.ntrain / opt.batch_size / opt.world_size)\n",
    "steps_val = ceil(opt.nval / opt.batch_size / opt.world_size)\n",
    "params_val = {'batch_size': opt.batch_size, 'shuffle': False}\n",
    "\n",
    "# loss function\n",
    "loss_type = ['loss']\n",
    "if opt.extra_loss: # None or string\n",
    "    extra_loss = opt.extra_loss.split('_')\n",
    "    extra_weight = [float(n) for n in opt.weight.split('_')]\n",
    "    loss_type = loss_type + extra_loss\n",
    "    if not len(extra_loss) == len(extra_weight):\n",
    "        raise Exception(f'Input {len(extra_loss)} weight with {len(extra_weight)} extra loss')\n",
    "print_log(f'[INFO==>] Loss types: {loss_type}',log)\n",
    "calc_loss = calculate_loss(opt,loss_type,extra_weight)\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "# start from scratch\n",
    "start_epoch, end_epoch = 0, max_epoch\n",
    "learning_results = {'val_max': [], 'val_sum': [], 'steps_per_epoch': steps_train}\n",
    "for loss in loss_type:\n",
    "    learning_results['train_'+loss] = []\n",
    "    learning_results['val_'+loss] = []\n",
    "best_val_loss = float('Inf')\n",
    "\n",
    "\n",
    "# starting time of training\n",
    "train_start_time = time.time()\n",
    "not_improve = 0\n",
    "\n",
    "print_log(f'[INFO==>] Start training from {start_epoch} to {end_epoch} rank {opt.rank}\\n',log)\n",
    "\n",
    "epoch = 0\n",
    "\n",
    "if opt.rank == 0:\n",
    "    print_log(f'Epoch {epoch+1}/{end_epoch} | {strftime(\"%Y-%m-%d %H:%M:%S\", localtime())} | lr {optimizer.param_groups[0][\"lr\"]}', log, arrow=True)\n",
    "\n",
    "# training phase\n",
    "model.train()\n",
    "metric, metrics = defaultdict(float), defaultdict(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Epoch 1/2 | 2022-10-16 20:30:03 | lr 0.001\n",
      "Epoch 0/1 Train 0/374 MaxOut 38.41 mse3d 55.6015  cel0 0.0010  loss 55.6025  \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 810.00 MiB (GPU 0; 10.76 GiB total capacity; 8.22 GiB already allocated; 432.44 MiB free; 9.46 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-14f1361c75cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupgrid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_upgrid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_im\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_ims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepstorm3d/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepstorm3d/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 810.00 MiB (GPU 0; 10.76 GiB total capacity; 8.22 GiB already allocated; 432.44 MiB free; 9.46 GiB reserved in total by PyTorch)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# starting time of current epoch\n",
    "epoch_start_time = time.time()\n",
    "\n",
    "if opt.rank == 0:\n",
    "    print_log(f'Epoch {epoch+1}/{end_epoch} | {strftime(\"%Y-%m-%d %H:%M:%S\", localtime())} | lr {optimizer.param_groups[0][\"lr\"]}', log, arrow=True)\n",
    "\n",
    "# training phase\n",
    "model.train()\n",
    "metric, metrics = defaultdict(float), defaultdict(float)\n",
    "with torch.set_grad_enabled(True):\n",
    "    for batch_ind, (inputs, targets, target_ims, fileids) in enumerate(training_generator):\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        target_ims = target_ims.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "        loss = calc_loss(upgrid=outputs.float(), gt_upgrid=targets, gt_im=target_ims, metric=metric, metrics=metrics)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        if opt.rank==0:\n",
    "            print(f'Epoch {epoch}/{end_epoch-1} Train {batch_ind}/{steps_train-1} MaxOut {outputs.max():.2f} {print_metric_format(metric)}')\n",
    "\n",
    "\n",
    "# if opt.scheduler_type == 'StepLR':\n",
    "    # scheduler.step()\n",
    "\n",
    "if opt.gpu_number:\n",
    "    for key in metrics.keys():\n",
    "        dist.all_reduce_multigpu([metrics[key]])\n",
    "\n",
    "# calculate and print all keys in metrics\n",
    "if opt.rank == 0:\n",
    "    print_metrics(metrics, opt.ntrain, 'Train', log)\n",
    "\n",
    "# record training loss\n",
    "for key in loss_type:\n",
    "    mean_train_tmp = (metrics[key]/opt.ntrain).cpu().numpy()\n",
    "    learning_results['train_'+key].append(mean_train_tmp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" validation \"\"\"\n",
    "model.eval()\n",
    "metrics_val = defaultdict(float)\n",
    "\n",
    "with torch.set_grad_enabled(False):\n",
    "    for batch_ind, (inputs, targets, target_ims, fileids) in enumerate(validation_generator):\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        target_ims = target_ims.to(device)\n",
    "\n",
    "        # forward\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "        val_loss = calc_loss(upgrid=outputs.float(), gt_upgrid=targets, gt_im=target_ims, metric=metric, metrics=metrics_val)\n",
    "\n",
    "        if opt.rank==0:\n",
    "            print(f'Epoch {epoch+1}/{end_epoch} Val {batch_ind}/{steps_val-1} MaxOut {outputs.max():.2f} {print_metric_format(metric)}')\n",
    "\n",
    "if opt.gpu_number:\n",
    "    for key in metrics_val.keys():\n",
    "        dist.all_reduce_multigpu([metrics_val[key]])\n",
    "\n",
    "# calculate and print all keys in metrics_val\n",
    "if opt.rank == 0:\n",
    "    print_metrics(metrics_val, opt.nval, 'Valid',log)\n",
    "\n",
    "# record validation loss\n",
    "for key in loss_type:\n",
    "    mean_train_tmp = (metrics_val[key]/opt.ntrain).cpu().numpy()\n",
    "    learning_results['val_'+key].append(mean_train_tmp)\n",
    "\n",
    "# if not opt.scheduler_type == 'StepLR':\n",
    "mean_val_loss = (metrics_val['loss']/opt.nval).cpu().numpy()\n",
    "scheduler.step(mean_val_loss)\n",
    "\n",
    "# sanity check: record maximal value and sum of last validation sample\n",
    "max_last = outputs.max().cpu().numpy()\n",
    "sum_last = (outputs.sum()/params_val['batch_size']).cpu().numpy()\n",
    "learning_results['val_max'].append(max_last)\n",
    "learning_results['val_sum'].append(sum_last)\n",
    "\n",
    "\n",
    "\n",
    "# visualize in tensorboard\n",
    "if opt.rank==0:\n",
    "    for key in metrics.keys():\n",
    "        logger.scalars_summary('LOSS/{}'.format(key) ,{'train':metrics[key]/opt.ntrain, 'val':metrics_val[key]/opt.nval}, epoch)\n",
    "    logger.scalar_summary('Other/MaxOut', max_last/opt.scaling_factor, epoch)\n",
    "    logger.scalar_summary('Other/MaxOutSum', sum_last, epoch)\n",
    "    logger.scalar_summary('Other/Learning Rate', optimizer.param_groups[0]['lr'], epoch)\n",
    "\n",
    "\n",
    "# save checkpoint\n",
    "if opt.rank==0:\n",
    "    # save latest checkpoint\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict()}, os.path.join(opt.save_path,'ckpt_latest'))\n",
    "    # save checkpoint per save_epoch\n",
    "    if epoch%opt.save_epoch == opt.save_epoch-1:\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()}, os.path.join(opt.save_path,'ckpt_'+str(epoch+1)))\n",
    "\n",
    "# save checkpoint for best val loss\n",
    "if mean_val_loss < (best_val_loss - 1e-4):\n",
    "    if opt.rank==0:\n",
    "        # print an update and save the model weights\n",
    "        print_log('Val loss improved from %.4f to %.4f, saving best model...'% (best_val_loss, mean_val_loss), log, arrow=True)\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()}, os.path.join(opt.save_path,'ckpt_best_loss'))\n",
    "    # change minimal loss and init stagnation indicator\n",
    "    best_val_loss = mean_val_loss\n",
    "    not_improve = 0\n",
    "else:\n",
    "    # update stagnation indicator\n",
    "    not_improve += 1\n",
    "    if opt.rank==0:\n",
    "        print_log('Val loss not improve by %d epochs, best val loss: %.4f'% (not_improve,best_val_loss), log, arrow=True)\n",
    "\n",
    "epoch_time_elapsed = time.time() - epoch_start_time\n",
    "if opt.rank==0:\n",
    "    print_log('Max test last: %.2f, Sum test last: %.2f' %(max_last, sum_last), log, arrow=True)\n",
    "    print_log('{}, Epoch complete in {}\\n'.format(time.ctime(time.time()),print_time(epoch_time_elapsed)), log, arrow=True)\n",
    "\n",
    "    # save all records for latter visualization\n",
    "    with open(os.path.join(opt.save_path, 'learning_results.pickle'), 'wb') as handle:\n",
    "        pickle.dump(learning_results, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Train mse3d: 0.0003, cel0: 0.0000, loss: 0.8709\n"
     ]
    }
   ],
   "source": [
    "if opt.gpu_number:\n",
    "    for key in metrics.keys():\n",
    "        dist.all_reduce_multigpu([metrics[key]])\n",
    "\n",
    "# calculate and print all keys in metrics\n",
    "if opt.rank == 0:\n",
    "    print_metrics(metrics, opt.ntrain, 'Train', log)\n",
    "\n",
    "# record training loss\n",
    "for key in loss_type:\n",
    "    mean_train_tmp = (metrics[key]/opt.ntrain).cpu().numpy()\n",
    "    learning_results['train_'+key].append(mean_train_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Functions \"\"\"\n",
    "# official modules\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.distributed as dist\n",
    "# self-defined modules\n",
    "from utils.networks import *\n",
    "\n",
    "def init_DDP(opt):\n",
    "    os.environ['MASTER_ADDR']='localhost'\n",
    "    os.environ['MASTER_PORT']='123458'\n",
    "    gpus = [g.strip() for g in opt.gpu_number.split(',')]\n",
    "    os.environ['CUDA_VISIBLE_DEVICES']=gpus[opt.rank]\n",
    "    dist.init_process_group('GLOO',rank=opt.rank,world_size=opt.world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_number = len(opt.gpu_number.split(','))\n",
    "# print(gpu_number)\n",
    "opt.rank = 0\n",
    "opt.world_size = 1\n",
    "init_DDP(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/hdd/lingjia/rPSF/result/CEL0/0613-lr0.0005-batchSize1-D250-Epoch30-nTrain9000-cnn_residual\n"
     ]
    }
   ],
   "source": [
    "tmp_train = np.arange(1,9001,1).tolist()\n",
    "tmp_val = np.arange(9001,10001,1).tolist()\n",
    "train_IDs = [str(i) for i in tmp_train]\n",
    "val_IDs = [str(i) for i in tmp_val]\n",
    "\n",
    "opt.partition = {'train': train_IDs, 'valid': val_IDs}\n",
    "opt.ntrain, opt.nval = len(train_IDs), len(val_IDs)\n",
    "\n",
    "# calculate zoom ratio of z-axis\n",
    "opt.pixel_size_axial = (opt.zmax - opt.zmin + 1 + 2*opt.clear_dist) / opt.D\n",
    "\n",
    "# output folder name for results\n",
    "t = time.strftime('%m%d') + \\\n",
    "    '-lr'+str(opt.initial_learning_rate) + \\\n",
    "    '-batchSize'+str(opt.batch_size) + \\\n",
    "    '-D'+str(opt.D) + \\\n",
    "    '-Epoch'+str(opt.max_epoch) + \\\n",
    "    '-nTrain'+str(opt.ntrain) + \\\n",
    "    '-'+str(opt.model_use)\n",
    "\n",
    "if opt.resume:\n",
    "    t = t + '-resume'\n",
    "if opt.postpro:\n",
    "    t = t + '-postpro'\n",
    "\n",
    "opt.save_path = os.path.join(opt.save_path,t)\n",
    "print(opt.save_path)\n",
    "# os.makedirs(opt.save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "# what is this?\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "if opt.rank==0:\n",
    "    # save setup parameters in result folder as well\n",
    "    with open(os.path.join(opt.save_path,'setup_params.json'),'w') as handle:\n",
    "        json.dump(opt.__dict__, handle, indent=2)\n",
    "\n",
    "# Load labels and generate dataset\n",
    "labels = load_labels(os.path.join(opt.data_path,'label.txt'))\n",
    "\n",
    "# Parameters for dataloaders\n",
    "params_train = {'batch_size': opt.batch_size, 'shuffle': True,  'partition': opt.partition['train']}\n",
    "params_val = {'batch_size': opt.batch_size, 'shuffle': False, 'partition': opt.partition['valid']}\n",
    "\n",
    "training_generator = dataloader(opt.data_path, labels, params_train, opt, num_workers=0)\n",
    "validation_generator = dataloader(opt.data_path, labels, params_val, opt, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spikes_pred = torch.zeros((96,96,41))\n",
    "spikes_pred[0,0,0] = 1\n",
    "\n",
    "norm_ai = torch.ones((96,96,41))\n",
    "\n",
    "# CEL0 on the predicted spikes\n",
    "norm_ai2 = tf.square(norm_ai)\n",
    "thresh = np.sqrt(2*weight)/norm_ai\n",
    "abs_heat = tf.abs(spikes_pred)\n",
    "bound = tf.square(abs_heat-thresh)\n",
    "ind = tf.cast(abs_heat <= thresh, tf.float32)\n",
    "loss_spikes = tf.reduce_mean((weight-0.5*(norm_ai2*bound)*ind))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CEL0Loss(nn.Module):\n",
    "    def __init__(self,norm_ai,weight):\n",
    "        super(CEL0Loss,self).__init__()\n",
    "        norm_ai = norm_ai\n",
    "        weight = weight\n",
    "\n",
    "    def forward(self,spikes_pred):\n",
    "        # CEL0 on the predicted spikes\n",
    "        norm_ai2 = torch.square(norm_ai)\n",
    "        thresh = np.sqrt(2*weight)/norm_ai\n",
    "        abs_heat = torch.abs(spikes_pred)\n",
    "        bound = torch.square(abs_heat-thresh)\n",
    "        ind =(abs_heat<= thresh).type(torch.float32)\n",
    "        loss_spikes = torch.mean((weight-0.5*(norm_ai2*bound)*ind))\n",
    "\n",
    "        return loss_spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cel0Loss = CEL0Loss(norm_ai=torch.ones(96,96,41),weight=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0796)\n"
     ]
    }
   ],
   "source": [
    "spikes_pred = torch.zeros((96,96,41))\n",
    "spikes_pred[:30,:,:20] = 1\n",
    "loss = cel0Loss(spikes_pred)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([96, 96, 41])\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "norm_ai_path = '/home/lingjia/Documents/rPSF/NN/utils/norm_ai.mat'\n",
    "norm_ai =  scipy.io.loadmat(norm_ai_path)['a']\n",
    "norm_ai =  torch.from_numpy(norm_ai)\n",
    "# print(type(norm_ai))\n",
    "print(norm_ai.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6309,  0.1307],\n",
      "        [ 1.9685, -1.3234]]) \n",
      " tensor([[[-0.3528, -1.1910],\n",
      "         [-0.7360, -0.5470]],\n",
      "\n",
      "        [[ 1.3621, -0.4500],\n",
      "         [ 2.7612, -0.5511]],\n",
      "\n",
      "        [[ 0.8429,  0.6193],\n",
      "         [-0.3980,  2.0948]]])\n"
     ]
    }
   ],
   "source": [
    "norm_ai = torch.randn((2,2))\n",
    "x = torch.randn((3,2,2))\n",
    "weight = 100\n",
    "print(norm_ai,'\\n',x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "tensor([[0.3981, 0.0171],\n",
      "        [3.8751, 1.7513]])\n"
     ]
    }
   ],
   "source": [
    "norm_ai2 = torch.square(norm_ai)\n",
    "print(norm_ai2.shape)\n",
    "print(norm_ai2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-22.4151, 108.2227],\n",
      "        [  7.1841, -10.6864]])\n"
     ]
    }
   ],
   "source": [
    "thresh = np.sqrt(2*weight)/norm_ai\n",
    "print(thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3528, 1.1910],\n",
      "         [0.7360, 0.5470]],\n",
      "\n",
      "        [[1.3621, 0.4500],\n",
      "         [2.7612, 0.5511]],\n",
      "\n",
      "        [[0.8429, 0.6193],\n",
      "         [0.3980, 2.0948]]])\n"
     ]
    }
   ],
   "source": [
    "abs_heat = torch.abs(x)\n",
    "print(abs_heat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  518.3761, 11455.7861],\n",
      "         [   41.5781,   126.1883]],\n",
      "\n",
      "        [[  565.3557, 11614.9531],\n",
      "         [   19.5626,   126.2795]],\n",
      "\n",
      "        [[  540.9315, 11578.5098],\n",
      "         [   46.0515,   163.3588]]])\n"
     ]
    }
   ],
   "source": [
    "bound = torch.square(abs_heat-thresh)\n",
    "print(bound)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [1., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "ind = (abs_heat<= thresh).type(torch.float32)\n",
    "print(ind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3981, 0.0171],\n",
      "        [3.8751, 1.7513]])\n",
      "tensor([[[  518.3761, 11455.7861],\n",
      "         [   41.5781,   126.1883]],\n",
      "\n",
      "        [[  565.3557, 11614.9531],\n",
      "         [   19.5626,   126.2795]],\n",
      "\n",
      "        [[  540.9315, 11578.5098],\n",
      "         [   46.0515,   163.3588]]])\n",
      "tensor([[[ 0.0000, 97.8110],\n",
      "         [80.5596,  0.0000]],\n",
      "\n",
      "        [[ 0.0000, 99.1700],\n",
      "         [37.9035,  0.0000]],\n",
      "\n",
      "        [[ 0.0000, 98.8589],\n",
      "         [89.2271,  0.0000]]])\n",
      "tensor([[[100.0000,   2.1890],\n",
      "         [ 19.4404, 100.0000]],\n",
      "\n",
      "        [[100.0000,   0.8300],\n",
      "         [ 62.0965, 100.0000]],\n",
      "\n",
      "        [[100.0000,   1.1411],\n",
      "         [ 10.7729, 100.0000]]])\n"
     ]
    }
   ],
   "source": [
    "print(norm_ai2)\n",
    "print(bound)\n",
    "print(0.5*(norm_ai2*bound)*ind)\n",
    "print((weight-0.5*(norm_ai2*bound)*ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(58.0392)\n"
     ]
    }
   ],
   "source": [
    "loss_spikes = torch.mean((weight-0.5*(norm_ai2*bound)*ind))\n",
    "print(loss_spikes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import torch\n",
    "import numpy as np\n",
    "# norm_ai_path = './matlab_codes/norm_ai_test.mat'dl\n",
    "# norm_ai = scipy.io.loadmat(norm_ai_path)['norm_ai_test']\n",
    "weight = 0.1\n",
    "# spikes_pred = torch.randn((96,96,21))\n",
    "\n",
    "norm_ai = torch.randn((2,2,3))\n",
    "spikes_pred = torch.randn((2,2,3))\n",
    "\n",
    "norm_ai2 = torch.square(norm_ai)\n",
    "thresh = np.sqrt(2*weight)/norm_ai\n",
    "abs_heat = torch.abs(spikes_pred)\n",
    "bound = torch.square(abs_heat-thresh)\n",
    "ind = (abs_heat<=thresh).type(torch.float32)\n",
    "loss_spikes = torch.mean((weight-0.5*(norm_ai2*bound)*ind))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm_ai:\n",
      "tensor([[[-0.5372, -0.7486, -0.6857],\n",
      "         [ 0.1539, -1.6697,  0.4003]],\n",
      "\n",
      "        [[-1.3305, -0.4892,  0.3958],\n",
      "         [ 0.0053, -0.0951,  0.9309]]])\n",
      "norm_ai2:\n",
      "tensor([[[2.8856e-01, 5.6036e-01, 4.7023e-01],\n",
      "         [2.3683e-02, 2.7878e+00, 1.6024e-01]],\n",
      "\n",
      "        [[1.7704e+00, 2.3927e-01, 1.5665e-01],\n",
      "         [2.8479e-05, 9.0473e-03, 8.6658e-01]]])\n"
     ]
    }
   ],
   "source": [
    "print(f'norm_ai:\\n{norm_ai}')\n",
    "print(f'norm_ai2:\\n{norm_ai2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_type = ['loss','mse3d','cel0']\n",
    "losses = ['train_'+n for n in loss_type]+['val_'+n for n in loss_type]\n",
    "# learning_results = {'train_loss': [], 'val_loss': [],\n",
    "#                     'train_mse3d': [], 'train_mse2d': [],\n",
    "#                     'train_dice': [], 'val_dice': [], \n",
    "#                     'train_mse2d': [], 'val_mse3d': [], \n",
    "#                     'train_CEL0': [], 'val_CEL0': [], \n",
    "#                     'val_max': [], 'val_sum': [], 'steps_per_epoch': steps_train}\n",
    "\n",
    "steps_train = 1\n",
    "learning_results = {'val_max': [], 'val_sum': [], 'steps_per_epoch': steps_train}\n",
    "\n",
    "for loss in losses:\n",
    "    learning_results[loss] = []\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: 0.1000  bb: 0.0970  \n"
     ]
    }
   ],
   "source": [
    "def print_metric(metric):\n",
    "    l = len(metric)\n",
    "    info = ''\n",
    "    for key in metric.keys():\n",
    "        info = info + f'{key}: {metric[key]:.4f}  '\n",
    "    print(info)\n",
    "\n",
    "metric = {'a':0.1,'bb':0.097}\n",
    "print_metric(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2]\n",
      " [3 4 5]\n",
      " [6 7 8]]\n",
      "(3, 3)\n",
      "[[[0 1 2 0 1 2]\n",
      "  [3 4 5 3 4 5]\n",
      "  [6 7 8 6 7 8]]]\n",
      "(1, 3, 6)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy\n",
    "a = torch.Tensor([[0,1,2],[3,4,5],[6,7,8]])\n",
    "a = numpy.array([[0,1,2],[3,4,5],[6,7,8]])\n",
    "print(a)\n",
    "print(a.shape)\n",
    "b = numpy.tile(a,(1,1,2))\n",
    "print(b)\n",
    "print(b.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 3])\n",
      "torch.Size([2, 3, 3])\n",
      "tensor([[[0., 1., 2.],\n",
      "         [3., 4., 5.],\n",
      "         [6., 7., 8.]],\n",
      "\n",
      "        [[0., 1., 2.],\n",
      "         [3., 4., 5.],\n",
      "         [6., 7., 8.]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([[0,1,2],[3,4,5],[6,7,8]])\n",
    "a = a.unsqueeze_(0)\n",
    "print(a.shape)\n",
    "b = a.expand(2,3,3)\n",
    "print(b.shape)\n",
    "print(b)\n",
    "# print(b[:,:,0])\n",
    "# print(b.permute(2,0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      "tensor([[[-0.1044, -1.3292, -0.6696],\n",
      "         [ 2.2702,  3.1764, -1.0143],\n",
      "         [-0.1016,  0.7238,  1.2969]],\n",
      "\n",
      "        [[-0.0660,  0.2672,  0.3975],\n",
      "         [-0.3368,  1.4430,  0.6608],\n",
      "         [ 1.2340,  1.1386,  1.3968]]])\n",
      "output:\n",
      "tensor([[[-0.0360, -0.7955, -0.2873],\n",
      "         [ 0.4308,  0.5143, -0.5108],\n",
      "         [-0.0350,  0.1944,  0.3018]],\n",
      "\n",
      "        [[-0.0225,  0.0818,  0.1170],\n",
      "         [-0.1265,  0.3248,  0.1805],\n",
      "         [ 0.2914,  0.2751,  0.3177]]])\n",
      "tensor(1.2158)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = 3\n",
    "input = torch.randn(2,3,3)\n",
    "print(f'input:\\n{input}')\n",
    "output = input/(a+input)\n",
    "print(f'output:\\n{output}')\n",
    "out = torch.sum(output)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-24-17-44-34\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'10'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = '/media/hdd/lingjia/hdd_rpsf/nonconvex_loss/temp/2022-09-24-17-44-34-lr0.001-bs16-D41-Ep5-nT90-w1.0-deepstorm3d'\n",
    "print(a.split('/')[-1][:])\n",
    "weight = a.split('w')[-1].split('-')[0]\n",
    "nc = 'klnc'\n",
    "weight+nc\n",
    "\n",
    "a = '/media/hdd/lingjia/hdd_rpsf/data/plain/test/test_plain/test10'\n",
    "a.split('/')[-1][4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022-10-08 13:58:02'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import localtime, strftime\n",
    "strftime(\"%Y-%m-%d %H:%M:%S\", localtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'mse3d', 'loss1', 'loss2', 'loss3']\n"
     ]
    }
   ],
   "source": [
    "loss_type = ['loss','mse3d']\n",
    "extra_loss = 'loss1_loss2_loss3'\n",
    "if extra_loss: # None or string\n",
    "    extra_loss = extra_loss.split('_')\n",
    "    loss_type = loss_type + extra_loss\n",
    "print(loss_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./matlab_codes/norm_ai_D127.mat\n",
      "(96, 96, 127)\n"
     ]
    }
   ],
   "source": [
    "import scipy.io\n",
    "D = 127\n",
    "# norm_ai_path = f'/home/lingjia/Documents/rpsf/3DLocalization/matlab_codes/norm_ai_D41.mat'\n",
    "# norm_ai_path = '/home/lingjia/Documents/rpsf/3DLocalization/matlab_codes/norm_ai.mat'\n",
    "norm_ai_path = f'./matlab_codes/norm_ai_D{D}.mat'\n",
    "print(norm_ai_path)\n",
    "norm_ai = scipy.io.loadmat(norm_ai_path)['norm_ai']\n",
    "print(norm_ai.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 ('deepstorm3d')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c40f20e5cf12c01a8510f5b704b157892e0eb193fc1e0d9c4d1938b744ef0863"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
