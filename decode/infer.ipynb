{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "from functools import partial\n",
    "from typing import Union, Callable\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from decode.neuralfitter import dataset\n",
    "from decode.generic import emitter\n",
    "from decode.utils import hardware, frames_io\n",
    "import decode.neuralfitter.utils\n",
    "\n",
    "\n",
    "def ship_device(x, device: Union[str, torch.device]):\n",
    "    \"\"\"\n",
    "    Ships the input to a pytorch compatible device (e.g. CUDA)\n",
    "\n",
    "    Args:\n",
    "        x:\n",
    "        device:\n",
    "\n",
    "    Returns:\n",
    "        x\n",
    "\n",
    "    \"\"\"\n",
    "    if x is None:\n",
    "        return x\n",
    "\n",
    "    elif isinstance(x, torch.Tensor):\n",
    "        return x.to(device)\n",
    "\n",
    "    elif isinstance(x, (tuple, list)):\n",
    "        x = [ship_device(x_el, device) for x_el in x]  # a nice little recursion that worked at the first try\n",
    "        return x\n",
    "\n",
    "    elif device != 'cpu':\n",
    "        raise NotImplementedError(f\"Unsupported data type for shipping from host to CUDA device.\")\n",
    "\n",
    "\n",
    "\n",
    "class Infer:\n",
    "\n",
    "    def __init__(self, model, ch_in: int, frame_proc, post_proc, device: Union[str, torch.device],\n",
    "                 batch_size: Union[int, str] = 'auto', num_workers: int = 0,\n",
    "                 pin_memory: bool = False, param = param,\n",
    "                 forward_cat: Union[str, Callable] = 'emitter'):\n",
    "        \"\"\"\n",
    "        Convenience class for inference.\n",
    "\n",
    "        Args:\n",
    "            model: pytorch model\n",
    "            ch_in: number of input channels\n",
    "            frame_proc: frame pre-processing pipeline\n",
    "            post_proc: post-processing pipeline\n",
    "            device: device where to run inference\n",
    "            batch_size: batch-size or 'auto' if the batch size should be determined automatically (only use in combination with cuda)\n",
    "            num_workers: number of workers\n",
    "            pin_memory: pin memory in dataloader\n",
    "            forward_cat: method which concatenates the output batches. Can be string or Callable.\n",
    "            Use 'em' when the post-processor outputs an EmitterSet, or 'frames' when you don't use post-processing or if\n",
    "            the post-processor outputs frames.\n",
    "        \"\"\"\n",
    "\n",
    "        self.model = model\n",
    "        self.ch_in = ch_in\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "        self.frame_proc = frame_proc\n",
    "        self.post_proc = post_proc\n",
    "\n",
    "        self.forward_cat = None\n",
    "        self._forward_cat_mode = forward_cat\n",
    "\n",
    "        if str(self.device) == 'cpu' and self.batch_size == 'auto':\n",
    "            warnings.warn(\n",
    "                \"Automatically determining the batch size does not make sense on cpu device. \"\n",
    "                \"Falling back to reasonable value.\")\n",
    "            self.batch_size = 64\n",
    "\n",
    "        self.loss = decode.neuralfitter.loss.GaussianMMLoss(\n",
    "                xextent=param.Simulation.psf_extent[0],\n",
    "                yextent=param.Simulation.psf_extent[1],\n",
    "                img_shape=param.Simulation.img_size,\n",
    "                device=device,\n",
    "                chweight_stat=param.HyperParameter.chweight_stat)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, ds) -> emitter.EmitterSet:\n",
    "        \"\"\"\n",
    "        Forward frames through model, pre- and post-processing and output EmitterSet\n",
    "\n",
    "        Args:\n",
    "            frames:\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"Move Model\"\"\"\n",
    "        model = self.model.to(self.device)\n",
    "        model.eval()\n",
    "\n",
    "        \"\"\"Form Dataset and Dataloader\"\"\"\n",
    "        # ds = dataset.InferenceDataset(frames=frames, frame_proc=self.frame_proc,\n",
    "        #                               frame_window=self.ch_in)\n",
    "\n",
    "        if self.batch_size == 'auto':\n",
    "            # include safety factor of 20%\n",
    "            # print(type(ds))\n",
    "            # print(ds[0])\n",
    "            bs = int(0.8 * self.get_max_batch_size(model, len(ds), 1, 512))\n",
    "            print(bs)\n",
    "        else:\n",
    "            bs = self.batch_size\n",
    "\n",
    "        # generate concatenate function here because we need batch size for this\n",
    "        self.forward_cat = self._setup_forward_cat(self._forward_cat_mode, bs)\n",
    "\n",
    "        dl = torch.utils.data.DataLoader(dataset=ds, batch_size=bs, shuffle=False, drop_last=False,\n",
    "                                         num_workers=self.num_workers, pin_memory=self.pin_memory,\n",
    "                                         collate_fn=decode.neuralfitter.utils.dataloader_customs.smlm_collate)\n",
    "\n",
    "        out = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # for batch_num, sample in enumerate(tqdm(dl)):\n",
    "            for batch_num, (x, y_tar, weight) in enumerate(tqdm(dl)):\n",
    "                # x_in = sample.to(self.device)\n",
    "                x, y_tar, weight = ship_device([x, y_tar, weight], self.device)\n",
    "\n",
    "                # compute output\n",
    "                y_out = model(x)\n",
    "\n",
    "                # loss computation\n",
    "                loss_val = self.loss(y_out, y_tar, weight)\n",
    "\n",
    "                \"\"\"Logging\"\"\"\n",
    "                loss_mean, loss_cmp = self.loss.log(loss_val)  # compute individual loss components\n",
    "                loss_gmm = loss_cmp['gmm']\n",
    "                loss_bg = loss_cmp['bg']\n",
    "\n",
    "\n",
    "                \"\"\"In post processing we need to make sure that we get a single Emitterset for each batch,\n",
    "                so that we can easily concatenate.\"\"\"\n",
    "                if self.post_proc is not None:\n",
    "                    out.append(self.post_proc.forward(y_out))\n",
    "                else:\n",
    "                    out.append(y_out.detach().cpu())\n",
    "\n",
    "                \"\"\"Cat to single emitterset / frame tensor depending on the specification of the forward_cat attr.\"\"\"\n",
    "                out = self.forward_cat(out)\n",
    "\n",
    "                return out\n",
    "\n",
    "    def _setup_forward_cat(self, forward_cat, batch_size: int):\n",
    "\n",
    "        if forward_cat is None:\n",
    "            return lambda x: x\n",
    "\n",
    "        elif isinstance(forward_cat, str):\n",
    "\n",
    "            if forward_cat == 'emitter':\n",
    "                return partial(emitter.EmitterSet.cat, step_frame_ix=batch_size)\n",
    "\n",
    "            elif forward_cat == 'frames':\n",
    "                return partial(torch.cat, dim=0)\n",
    "\n",
    "        elif callable(forward_cat):\n",
    "            return forward_cat\n",
    "\n",
    "        else:\n",
    "            raise TypeError(f\"Specified forward cat method was wrong.\")\n",
    "\n",
    "        raise ValueError(f\"Unsupported forward_cat value.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def get_max_batch_size(model: torch.nn.Module, frame_size: Union[tuple, torch.Size],\n",
    "                           limit_low: int, limit_high: int):\n",
    "        \"\"\"\n",
    "        Get maximum batch size for inference.\n",
    "\n",
    "        Args:\n",
    "            model: model on correct device\n",
    "            frame_size: size of frames (without batch dimension)\n",
    "            limit_low: lower batch size limit\n",
    "            limit_high: upper batch size limit\n",
    "        \"\"\"\n",
    "\n",
    "        def model_forward_no_grad(x: torch.Tensor):\n",
    "            \"\"\"\n",
    "            Helper function because we need to account for torch.no_grad()\n",
    "            \"\"\"\n",
    "            with torch.no_grad():\n",
    "                o = model.forward(x)\n",
    "\n",
    "            return o\n",
    "\n",
    "        assert next(model.parameters()).is_cuda, \\\n",
    "            \"Auto determining the max batch size makes only sense when running on CUDA device.\"\n",
    "\n",
    "        return hardware.get_max_batch_size(model_forward_no_grad, frame_size,\n",
    "                                           next(model.parameters()).device,\n",
    "                                           limit_low, limit_high)\n",
    "\n",
    "\n",
    "class LiveInfer(Infer):\n",
    "    def __init__(self,\n",
    "                 model, ch_in: int, *,\n",
    "                 stream, time_wait=5, safety_buffer: int = 20,\n",
    "                 frame_proc=None, post_proc=None,\n",
    "                 device: Union[\n",
    "                     str, torch.device] = 'cuda:0' if torch.cuda.is_available() else 'cpu',\n",
    "                 batch_size: Union[int, str] = 'auto', num_workers: int = 0,\n",
    "                 pin_memory: bool = False,\n",
    "                 forward_cat: Union[str, Callable] = 'emitter'):\n",
    "        \"\"\"\n",
    "        Inference from memmory mapped tensor, where the mapped file is possibly live being written to.\n",
    "\n",
    "        Args:\n",
    "            model: pytorch model\n",
    "            ch_in: number of input channels\n",
    "            stream: output stream. Will typically get emitters (along with starting and stopping index)\n",
    "            time_wait: wait if length of mapped tensor has not changed\n",
    "            safety_buffer: buffer distance to end of tensor to avoid conflicts when the file is actively being\n",
    "            written to\n",
    "            frame_proc: frame pre-processing pipeline\n",
    "            post_proc: post-processing pipeline\n",
    "            device: device where to run inference\n",
    "            batch_size: batch-size or 'auto' if the batch size should be determined automatically (only use in combination with cuda)\n",
    "            num_workers: number of workers\n",
    "            pin_memory: pin memory in dataloader\n",
    "            forward_cat: method which concatenates the output batches. Can be string or Callable.\n",
    "            Use 'em' when the post-processor outputs an EmitterSet, or 'frames' when you don't use post-processing or if\n",
    "            the post-processor outputs frames.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(\n",
    "            model=model, ch_in=ch_in, frame_proc=frame_proc, post_proc=post_proc,\n",
    "            device=device, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory,\n",
    "            forward_cat=forward_cat)\n",
    "\n",
    "        self._stream = stream\n",
    "        self._time_wait = time_wait\n",
    "        self._buffer_length = safety_buffer\n",
    "\n",
    "    def forward(self, frames: Union[torch.Tensor, frames_io.TiffTensor]):\n",
    "\n",
    "        n_fitted = 0\n",
    "        n_waited = 0\n",
    "        while n_waited <= 2:\n",
    "            n = len(frames)\n",
    "\n",
    "            if n_fitted == n - self._buffer_length:\n",
    "                n_waited += 1\n",
    "                time.sleep(self._time_wait)  # wait\n",
    "                continue\n",
    "\n",
    "            n_2fit = n - self._buffer_length\n",
    "            out = super().forward(frames[n_fitted:n_2fit])\n",
    "            self._stream(out, n_fitted, n_2fit)\n",
    "\n",
    "            n_fitted = n_2fit\n",
    "            n_waited = 0\n",
    "\n",
    "        # fit remaining frames\n",
    "        out = super().forward(frames[n_fitted:n])\n",
    "        self._stream(out, n_fitted, n)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
