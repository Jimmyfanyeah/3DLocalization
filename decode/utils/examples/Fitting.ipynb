{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECODE - Fit SMLM Data\n",
    "The purpose of this notebook is to demonstrate the fitting procedure for a pretrained model.\n",
    "Please be advised to have a read of the Introduction notebook first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T12:51:13.279039Z",
     "start_time": "2021-03-17T12:51:09.146649Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import decode\n",
    "import decode.utils\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "print(f\"DECODE version: {decode.utils.bookkeeping.decode_state()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters\n",
    "Set device for inference (i.e. CUDA vs. CPU, for our setup inference on the CPU is about 10 times slower). If you fit on CPU though, you may want to change the number of threads if you have a big machine (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T12:51:13.379796Z",
     "start_time": "2021-03-17T12:51:13.343661Z"
    }
   },
   "outputs": [],
   "source": [
    "device = 'cuda:0'  # or 'cpu', or you change cuda device index\n",
    "threads = 4  #  number of threads, useful for CPU heavy computation. Change if you know what you are doing.\n",
    "worker = 0  # number of workers for data loading. Used a default of 0 for safety. Multiprocessing on windows is sometimes not stable\n",
    "\n",
    "torch.set_num_threads(threads)  # set num threads\n",
    "\n",
    "if device != 'cpu':\n",
    "    if not torch.cuda.is_available():\n",
    "        raise ValueError(\"You have selected a non CPU device, but CUDA is not available.\"\n",
    "                         \"Refer to CPU version or check your installation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Specify paths for the model, parameters and frames\n",
    "If you trained you own model (using the Training notebook) you can specify it's path here.\n",
    "\n",
    "**Important** If the camera parameters of the training differ from the data which should be fitted (e.g. different EM gain), you can try to use the model anyways, but you must specify them here since we convert to photon units before forwarding through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T12:51:13.418196Z",
     "start_time": "2021-03-17T12:51:13.381779Z"
    }
   },
   "outputs": [],
   "source": [
    "# here you need to specify the parameters with suffix _run.yaml in your model's output folder (not param_run_in.yaml)\n",
    "param_path = 'config/param_run.yaml'\n",
    "model_path = 'network/model_0.pt'\n",
    "frame_path = 'experimental_data_workflow/frames.tif'\n",
    "\n",
    "# specify camera parameters of tiffs\n",
    "meta = {\n",
    "    'Camera': {\n",
    "        'baseline': 398.6,\n",
    "        'e_per_adu': 5.0,\n",
    "        'em_gain': 50.0,\n",
    "        'spur_noise': 0.0015  # if you don't know, you can set this to 0.\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T14:36:41.999701Z",
     "start_time": "2020-10-10T14:36:41.990217Z"
    }
   },
   "source": [
    "Alternatively if you just want to check out the fitting procedure we provide several example data for trying out DECODE. For this we load a gateway file which includes the links to the respective data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T12:51:16.631828Z",
     "start_time": "2021-03-17T12:51:13.420381Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gateway = decode.utils.example_helper.load_gateway()\n",
    "\n",
    "# dir where to store example data, leave as '' to store in current folder\n",
    "path = Path('')\n",
    "\n",
    "# change here for other files\n",
    "package = gateway['examples']['experimental_data_workflow']\n",
    "\n",
    "# get paths to files\n",
    "zip_folder = decode.utils.example_helper.load_example_package(\n",
    "    path=(path / package['name']).with_suffix('.zip'), url=package['url'], hash=package['hash'])\n",
    "\n",
    "frame_path = zip_folder / 'frames.tif'\n",
    "meta_path = zip_folder / 'frames_meta.yaml'\n",
    "model_path = zip_folder / 'model.pt'\n",
    "param_path = zip_folder / 'param_run.yaml'\n",
    "\n",
    "# load meta information (em gain of tif etc.)\n",
    "with meta_path.open('r') as f:\n",
    "    meta = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Parameters and Model\n",
    "Specify Post-Processing as by the parameter file you trained the model with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T12:51:18.883301Z",
     "start_time": "2021-03-17T12:51:16.634147Z"
    }
   },
   "outputs": [],
   "source": [
    "param = decode.utils.param_io.load_params(param_path)\n",
    "model = decode.neuralfitter.models.SigmaMUNet.parse(param)\n",
    "model = decode.utils.model_io.LoadSaveModel(model,\n",
    "                                            input_file=model_path,\n",
    "                                            output_file=None).load_init(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T12:51:18.918424Z",
     "start_time": "2021-03-17T12:51:18.885789Z"
    }
   },
   "outputs": [],
   "source": [
    "# overwrite camera\n",
    "param = decode.utils.param_io.autofill_dict(meta['Camera'], param.to_dict(), mode_missing='include')\n",
    "param = decode.utils.param_io.RecursiveNamespace(**param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Process Frames\n",
    "Load TIFF file.\n",
    "Change only needed if you load a custom tif file.\n",
    "Note that the TIFF loader will auto-load and concatenate tiff files\n",
    "that are consecutive and share\n",
    "the same meta data. For example specifying `dummy.tif` will also load  `dummy_0.tif, dummy_1.tif` if present in the\n",
    "same folder.\n",
    "If you have single page tiff files, you can also specify a folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T12:51:22.117010Z",
     "start_time": "2021-03-17T12:51:18.921202Z"
    }
   },
   "outputs": [],
   "source": [
    "# depends on your input, e.g. load a tiff\n",
    "frames = decode.utils.frames_io.load_tif(frame_path)\n",
    "\n",
    "camera = decode.simulation.camera.Photon2Camera.parse(param)\n",
    "camera.device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Pre- and Post-Processing\n",
    "In the following we will set up the processing pipeline.\n",
    "\n",
    "### Frame-Processing\n",
    "**WARNING: Mirror2D** Depending on your specific camera setup you need to modify the following pipeline. In our case (using an EM-CCD camera) the beads are acquired without EM-Gain, while the experimental frames were acquired with EM-Gain. For our camera, due to some hardware specifics, EM-Gain enabled will lead the frames to be mirrored in the last dimension. Since the trained model and the experimental frames need to be consistent with respect to the PSF, we account for this mirroring by `Mirror2D(dims=-1)`.\n",
    "\n",
    "Depending on your individual camera you might be able to omit this. Particularly if you are using a sCMOS camera.\n",
    "\n",
    "**AutoCenterCrop** Due to peculiarities of training a Deep Learning model, the size of the input frame is slightly restricted. In our case the input frame size must be in multitudes of 8 per edge. For this reason we ***center crop*** the frame to a size that satisfies this requirement. That means an input frame of size 34 x 41 would be cropped to 32 x 40 and only fitted in this region. If you want to use a non-destructive method you could replace `AutoCenterCrop` by `AutoPad`, however this could lead to a distortion at the frame border which is why we recommend `AutoCenterCrop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T12:51:22.181502Z",
     "start_time": "2021-03-17T12:51:22.119953Z"
    }
   },
   "outputs": [],
   "source": [
    "# setup frame processing as by the parameter with which the model was trained\n",
    "frame_proc = decode.neuralfitter.utils.processing.TransformSequence([\n",
    "    decode.neuralfitter.utils.processing.wrap_callable(camera.backward),\n",
    "    decode.neuralfitter.frame_processing.AutoCenterCrop(8),\n",
    "    decode.neuralfitter.frame_processing.Mirror2D(dims=-1),  # WARNING: You might need to comment this line out. see above\n",
    "    decode.neuralfitter.scale_transform.AmplitudeRescale.parse(param)\n",
    "])\n",
    "\n",
    "# determine extent of frame and its dimension after frame_processing\n",
    "size_procced = decode.neuralfitter.frame_processing.get_frame_extent(frames.unsqueeze(1).size(), frame_proc.forward)  # frame size after processing\n",
    "frame_extent = ((-0.5, size_procced[-2] - 0.5), (-0.5, size_procced[-1] - 0.5))\n",
    "\n",
    "\n",
    "# Setup post-processing\n",
    "# It's a sequence of backscaling, relative to abs. coord conversion and frame2emitter conversion\n",
    "post_proc = decode.neuralfitter.utils.processing.TransformSequence([\n",
    "\n",
    "    decode.neuralfitter.scale_transform.InverseParamListRescale.parse(param),\n",
    "\n",
    "    decode.neuralfitter.coord_transform.Offset2Coordinate(xextent=frame_extent[0],\n",
    "                                                          yextent=frame_extent[1],\n",
    "                                                          img_shape=size_procced[-2:]),\n",
    "\n",
    "    decode.neuralfitter.post_processing.SpatialIntegration(raw_th=0.1,\n",
    "                                                          xy_unit='px',\n",
    "                                                          px_size=param.Camera.px_size)\n",
    "\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T12:53:14.032842Z",
     "start_time": "2021-03-17T12:51:22.183390Z"
    }
   },
   "outputs": [],
   "source": [
    "infer = decode.neuralfitter.Infer(model=model, ch_in=param.HyperParameter.channels_in,\n",
    "                                  frame_proc=frame_proc, post_proc=post_proc,\n",
    "                                  device=device, num_workers=worker)\n",
    "\n",
    "emitter = infer.forward(frames[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check on the output\n",
    "print(emitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check if the predictions look reasonable on a random frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T12:53:15.356649Z",
     "start_time": "2021-03-17T12:53:14.035621Z"
    }
   },
   "outputs": [],
   "source": [
    "random_ix = torch.randint(frames.size(0), size=(1, )).item()\n",
    "\n",
    "em_subset = emitter.get_subset_frame(random_ix, random_ix)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "decode.plot.PlotFrameCoord(frame=frame_proc.forward(frames[[random_ix]])).plot()\n",
    "\n",
    "plt.subplot(122)\n",
    "decode.plot.PlotFrameCoord(frame=frame_proc.forward(frames[[random_ix]]),\n",
    "                           pos_out=em_subset.xyz_px, phot_out=em_subset.prob).plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compare the inferred distribution of the photon numbers and background values with the ranges used during training.\n",
    "If the inferred values fall outside of the green regions, or are concentrated in a small subspace of it, it might make sense to adjust the simulation parameters and retrain the the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T12:53:19.811545Z",
     "start_time": "2021-03-17T12:53:15.363086Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 4))\n",
    "\n",
    "plt.subplot(131)\n",
    "mu, sig = param.Simulation.intensity_mu_sig\n",
    "plt.axvspan(0, mu + sig * 3, color='green', alpha=0.1)\n",
    "sns.distplot(emitter.phot.numpy())\n",
    "plt.xlabel('Inferred number of photons')\n",
    "plt.xlim(0)\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.axvspan(*param.Simulation.bg_uniform, color='green', alpha=0.1)\n",
    "sns.distplot(emitter.bg.numpy())\n",
    "plt.xlabel('Inferred background values')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DECODE will in general detect very high numbers of emitters, including those with high uncertainties.\n",
    "Fortunately every DECODE prediction includes uncertainty estimates in x,y and z which we plot here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T12:53:26.764737Z",
     "start_time": "2021-03-17T12:53:19.813816Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,4))\n",
    "plt.subplot(131)\n",
    "sns.distplot(emitter.xyz_sig_nm[:, 0].numpy())\n",
    "plt.xlabel('Sigma Estimate in X (nm)')\n",
    "\n",
    "plt.subplot(132)\n",
    "sns.distplot(emitter.xyz_sig_nm[:, 1].numpy())\n",
    "plt.xlabel('Sigma Estimate in Y (nm)')\n",
    "\n",
    "plt.subplot(133)\n",
    "sns.distplot(emitter.xyz_sig_nm[:, 2].numpy())\n",
    "plt.xlabel('Sigma Estimate in Z (nm)')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a simple histogram rendering procedure for initial evaluation. You can specify the pixel size in nm, the sigma value for a Gaussian blur, and a clipping value and contrast to control the brightness.\n",
    "The histogram takes an Emitterset as input, and optionally also a vector of the same length which adds a color dimension.\n",
    "Below we plot the x-y view with z as colordimension as well as the x-z view.\n",
    "On the right side we instead color by the combined inferred uncertainty. This highlights some artifacts (red spots) and how the uncertainty is driven by the z location and the density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T12:59:29.183606Z",
     "start_time": "2021-03-17T12:59:08.365003Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(24, 12), sharex='col',\n",
    "                        gridspec_kw={'height_ratios': [1, 1200 / 20000]})\n",
    "\n",
    "decode.renderer.Renderer2D(px_size=10., sigma_blur=5., rel_clip=None, abs_clip=5,\n",
    "                           zextent=[-600, 600], colextent=[-500, 500], plot_axis=(0, 1),\n",
    "                           contrast=1.25).render(emitter, emitter.xyz_nm[:, 2], ax=axs[0, 0])\n",
    "decode.renderer.Renderer2D(px_size=10., sigma_blur=5., rel_clip=None, abs_clip=50,\n",
    "                           zextent=[-600, 600], plot_axis=(0, 2)).render(emitter, ax=axs[1, 0])\n",
    "\n",
    "decode.renderer.Renderer2D(px_size=10., sigma_blur=5., rel_clip=None, abs_clip=5,\n",
    "                           zextent=[-600, 600], colextent=[0, 75], plot_axis=(0, 1),\n",
    "                           contrast=1.25).render(emitter, emitter.xyz_sig_weighted_tot_nm, ax=axs[0, 1])\n",
    "\n",
    "decode.renderer.Renderer2D(px_size=10., sigma_blur=5., rel_clip=None, abs_clip=50,\n",
    "                           zextent=[-600, 600], colextent=[0, 75], plot_axis=(0, 2)).\\\n",
    "    render(emitter, emitter.xyz_sig_weighted_tot_nm, ax=axs[1, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is almost always beneficial for the image quality to remove the worst localizations.\n",
    "Below we remove all localizations with uncertainties that exceed 40 nm in x,y or 80 nm in z.\n",
    "This leaves us with 74% of the detections. Alternatively you can specify the remaining percentage directly with the `emitter.filter_by_sigma(fraction)` method which will remove the emitters with the lowest combined uncertainty.\n",
    "If you compare the rendering to the previous (unfiltered) one, you can see that this procedure eliminated all visible artefacts. However it also removed out of focus tubules in the top right corner that where previously visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T12:59:37.738395Z",
     "start_time": "2021-03-17T12:59:29.186189Z"
    }
   },
   "outputs": [],
   "source": [
    "sigma_x_high_threshold = 40\n",
    "sigma_y_high_threshold = 40\n",
    "sigma_z_high_threshold = 80\n",
    "\n",
    "em_sub = emitter[\n",
    "    (emitter.xyz_sig_nm[:, 0] <= sigma_x_high_threshold)\n",
    "    * (emitter.xyz_sig_nm[:, 1] <= sigma_x_high_threshold)\n",
    "    * (emitter.xyz_sig_nm[:, 2] <= sigma_z_high_threshold)\n",
    "    ]\n",
    "# em_sub = emitter.filter_by_sigma(0.67)  # alternatively\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "decode.renderer.Renderer2D(px_size=10., sigma_blur=5., rel_clip=None, abs_clip=5,\n",
    "                           zextent=[-600, 600], colextent=[-500, 500], plot_axis=(0, 1),\n",
    "                           contrast=1.5).render(em_sub, em_sub.xyz_nm[:, 2])\n",
    "plt.title(\n",
    "    f'Filtered Image {np.round(100 * len(em_sub) / len(emitter))} % of em_subs remaining', loc='right')\n",
    "plt.show()\n",
    "plt.figure(figsize=(12, 3))\n",
    "decode.renderer.Renderer2D(px_size=10., sigma_blur=5., rel_clip=None, abs_clip=50,\n",
    "                           zextent=[-600, 600], plot_axis=(0, 2)).render(em_sub)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of removing localizations you can also plot all of them and account for their uncertainty by rendering every localization as a Gaussian with a two dimensional standard deviation equal to the predicted uncertainty.\n",
    "Below we compare plotting with a constant sigma blur, and with individually rendered Gaussians. The second option removes artifacts and correctly blurs out of focus structures.\n",
    "\n",
    "More advanced plotting routines are for example available in the SMAP package (https://github.com/jries/SMAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T13:00:41.209122Z",
     "start_time": "2021-03-17T12:59:37.740896Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(24, 12), sharex='col',\n",
    "                        gridspec_kw={'height_ratios': [1, 1200 / 7000]})\n",
    "extents = {\n",
    "    'xextent': [14000, 22000],\n",
    "    'yextent': [1000, 8000],\n",
    "    'zextent': [-600, 600],\n",
    "    'colextent': [-500, 500]\n",
    "}\n",
    "\n",
    "decode.renderer.Renderer2D(\n",
    "    px_size=5., sigma_blur=5., rel_clip=None, abs_clip=3, **extents,\n",
    "    plot_axis=(0, 1), contrast=3).render(emitter, emitter.xyz_nm[:, 2], ax=axs[0, 0])\n",
    "\n",
    "decode.renderer.Renderer2D(\n",
    "    px_size=5., sigma_blur=5., rel_clip=None, abs_clip=15, **extents,\n",
    "    plot_axis=(0, 2), contrast=2).render(emitter, ax=axs[1, 0])\n",
    "\n",
    "decode.renderer.RendererIndividual2D(\n",
    "    px_size=5., filt_size=20, rel_clip=None, abs_clip=3, **extents,\n",
    "    plot_axis=(0, 1), contrast=3).render(emitter, emitter.xyz_nm[:, 2], ax=axs[0, 1])\n",
    "\n",
    "decode.renderer.RendererIndividual2D(\n",
    "    px_size=5., filt_size=20, rel_clip=None, abs_clip=3, **extents,\n",
    "    plot_axis=(0, 2)).render(emitter, ax=axs[1, 1])\n",
    "\n",
    "axs[0, 0].set_title('Rendering with constant sigma blur 5 nm', fontsize=20)\n",
    "axs[0, 1].set_title('Rendering with individual sigmas', fontsize=20)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save EmitterSet\n",
    "We support `.h5 .csv and .pt` to save an EmitterSet to file. Please consult the Introduction notebook for more\n",
    "information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T12:53:27.772816Z",
     "start_time": "2021-03-17T12:51:08.969Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "emitter.save('emitter.h5')  # or '.csv' or '.pt'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:decode_dev]",
   "language": "python",
   "name": "conda-env-decode_dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}