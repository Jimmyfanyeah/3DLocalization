{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lingjia/anaconda3/envs/decode_dev/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import datetime\n",
    "import os\n",
    "import shutil\n",
    "import socket\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "import decode.evaluation\n",
    "import decode.neuralfitter\n",
    "import decode.neuralfitter.coord_transform\n",
    "import decode.neuralfitter.utils\n",
    "import decode.simulation\n",
    "import decode.utils\n",
    "from decode.neuralfitter.train.random_simulation import setup_random_simulation\n",
    "from decode.neuralfitter.utils import log_train_val_progress\n",
    "from decode.utils.checkpoint import CheckPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device_overwrite = 'cuda'\n",
    "# debug = False\n",
    "# num_worker_override = None\n",
    "# no_log = False\n",
    "# log_folder = '/home/lingjia/Documents/rPSF/log'\n",
    "# log_comment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load Parameters and back them up to the network output directory\"\"\"\n",
    "param_file = '/home/lingjia/Documents/rPSF/NN/param_run_in.yaml'\n",
    "# param_file = '/home/lingjia/Documents/rPSF/NN/param.yaml'\n",
    "param_file = Path(param_file)\n",
    "param = decode.utils.param_io.ParamHandling().load_params(param_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(param.Simulation.img_size))\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Training Args')\n",
    "\n",
    "parser.add_argument('-i', '--device', default=None, \n",
    "                    help='Specify the device string (cpu, cuda, cuda:0) and overwrite param.',\n",
    "                    type=str)\n",
    "\n",
    "parser.add_argument('-p', '--param_file', default=None,\n",
    "                    help='Specify your parameter file (.yml or .json).', type=str)\n",
    "\n",
    "# parser.add_argument('-d', '--debug', default=False, action='store_true',\n",
    "#                     help='Debug the specified parameter file. Will reduce ds size for example.')\n",
    "\n",
    "parser.add_argument('-w', '--num_worker_override',default=None,\n",
    "                    help='Override the number of workers for the dataloaders.',\n",
    "                    type=int)\n",
    "\n",
    "parser.add_argument('-n', '--no_log', default=False, action='store_true',\n",
    "                    help='Set no log if you do not want to log the current run.')\n",
    "\n",
    "# parser.add_argument('-l', '--log_folder', default='runs',\n",
    "#                     help='Specify the (parent) folder you want to log to. If rel-path, relative to DECODE root.')\n",
    "\n",
    "parser.add_argument('-c', '--log_comment', default=None,\n",
    "                    help='Add a log_comment to the run.')\n",
    "\n",
    "parser.add_argument('-d', '--data_path_override', default=None,\n",
    "                    help='Specify your path to data', type=str)\n",
    "\n",
    "parser.add_argument('-is', '--img_size_override', default=None,\n",
    "                    help='Override img size', type=list)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.5, 39.5], [-0.5, 39.5], None]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(param.Simulation.psf_extent[0])\n",
    "param.Simulation.psf_extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "xextent = [-0.5,39.5]\n",
    "yextent = [-0.5,39.5]\n",
    "img_size = [40,40]\n",
    "\n",
    "bin_x = torch.linspace(*xextent, steps=img_size[0] + 1)\n",
    "bin_y = torch.linspace(*yextent, steps=img_size[1] + 1)\n",
    "bin_ctr_x = (bin_x + (bin_x[1] - bin_x[0]) / 2)[:-1]\n",
    "bin_ctr_y = (bin_y + (bin_y[1] - bin_y[0]) / 2)[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5000,  0.5000,  1.5000,  2.5000,  3.5000,  4.5000,  5.5000,  6.5000,\n",
      "         7.5000,  8.5000,  9.5000, 10.5000, 11.5000, 12.5000, 13.5000, 14.5000,\n",
      "        15.5000, 16.5000, 17.5000, 18.5000, 19.5000, 20.5000, 21.5000, 22.5000,\n",
      "        23.5000, 24.5000, 25.5000, 26.5000, 27.5000, 28.5000, 29.5000, 30.5000,\n",
      "        31.5000, 32.5000, 33.5000, 34.5000, 35.5000, 36.5000, 37.5000, 38.5000,\n",
      "        39.5000])\n",
      "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
      "        14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26., 27.,\n",
      "        28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38., 39.])\n"
     ]
    }
   ],
   "source": [
    "print(bin_x)\n",
    "print(bin_ctr_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RecursiveNamespace' object has no attribute 'Meta'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/lingjia/Documents/rPSF/NN/main.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B144.214.74.91/home/lingjia/Documents/rPSF/NN/main.ipynb#ch0000005vscode-remote?line=0'>1</a>\u001b[0m \u001b[39m# auto-set some parameters (will be stored in the backup copy)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B144.214.74.91/home/lingjia/Documents/rPSF/NN/main.ipynb#ch0000005vscode-remote?line=1'>2</a>\u001b[0m \u001b[39m# param = decode.utils.param_io.autoset_scaling(param)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B144.214.74.91/home/lingjia/Documents/rPSF/NN/main.ipynb#ch0000005vscode-remote?line=2'>3</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B144.214.74.91/home/lingjia/Documents/rPSF/NN/main.ipynb#ch0000005vscode-remote?line=3'>4</a>\u001b[0m \u001b[39m# add meta information\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B144.214.74.91/home/lingjia/Documents/rPSF/NN/main.ipynb#ch0000005vscode-remote?line=4'>5</a>\u001b[0m param\u001b[39m.\u001b[39;49mMeta\u001b[39m.\u001b[39mversion \u001b[39m=\u001b[39m decode\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mbookkeeping\u001b[39m.\u001b[39mdecode_state()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RecursiveNamespace' object has no attribute 'Meta'"
     ]
    }
   ],
   "source": [
    "# auto-set some parameters (will be stored in the backup copy)\n",
    "# param = decode.utils.param_io.autoset_scaling(param)\n",
    "\n",
    "# add meta information\n",
    "param.Meta.version = decode.utils.bookkeeping.decode_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-02_12-01-47_user-WS-C621E-SAGE-Series\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Experiment ID\"\"\"\n",
    "if not debug:\n",
    "    if param.InOut.checkpoint_init is None:\n",
    "        experiment_id = datetime.datetime.now().strftime(\n",
    "            \"%Y-%m-%d_%H-%M-%S\") + '_' + socket.gethostname()\n",
    "        from_ckpt = False\n",
    "        if log_comment:\n",
    "            experiment_id = experiment_id + '_' + log_comment\n",
    "    else:\n",
    "        from_ckpt = True\n",
    "        experiment_id = Path(param.InOut.checkpoint_init).parent.name\n",
    "else:\n",
    "    experiment_id = 'debug'\n",
    "    from_ckpt = False\n",
    "print(experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/hdd/rPSF_results/decode_impl\n",
      "/media/hdd/rPSF_results/decode_impl/2022-04-02_12-01-47_user-WS-C621E-SAGE-Series\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Set up unique folder for experiment\"\"\"\n",
    "if not from_ckpt:\n",
    "    experiment_path = Path(param.InOut.experiment_out) / Path(experiment_id)\n",
    "else:\n",
    "    experiment_path = Path(param.InOut.checkpoint_init).parent\n",
    "print(param.InOut.experiment_out)\n",
    "print(experiment_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/hdd/rPSF_results/decode_impl/2022-04-02_12-01-47_user-WS-C621E-SAGE-Series/model.pt\n",
      "/media/hdd/rPSF_results/decode_impl/2022-04-02_12-01-47_user-WS-C621E-SAGE-Series/ckpt.pt\n"
     ]
    }
   ],
   "source": [
    "model_out = experiment_path / Path('model.pt')\n",
    "ckpt_path = experiment_path / Path('ckpt.pt')\n",
    "print(model_out)\n",
    "print(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "_, device_ix = decode.utils.hardware._specific_device_by_str(device)\n",
    "print(device_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lingjia/Documents/rPSF/log/2022-04-02_12-01-47_user-WS-C621E-SAGE-Series\n"
     ]
    }
   ],
   "source": [
    "log_folder = log_folder + '/' + experiment_id\n",
    "print(log_folder)\n",
    "\n",
    "logger = decode.neuralfitter.utils.logger.MultiLogger(\n",
    "    [decode.neuralfitter.utils.logger.SummaryWriter(log_dir=log_folder,\n",
    "                                                    filter_keys=[\"dx_red_mu\", \"dx_red_sig\",\n",
    "                                                                    \"dy_red_mu\",\n",
    "                                                                    \"dy_red_sig\", \"dz_red_mu\",\n",
    "                                                                    \"dz_red_sig\",\n",
    "                                                                    \"dphot_red_mu\",\n",
    "                                                                    \"dphot_red_sig\"]),\n",
    "        decode.neuralfitter.utils.logger.DictLogger()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<decode.simulation.simulator.Simulation object at 0x7fb33842b0d0>\n"
     ]
    }
   ],
   "source": [
    "sim_train, sim_test = setup_random_simulation(param)\n",
    "print(sim_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdamW\n"
     ]
    }
   ],
   "source": [
    "# visualize cell\n",
    "# print(param)\n",
    "# print(param.InOut.checkpoint_init)\n",
    "# print(experiment_id)\n",
    "# print(param.Hardware.device)\n",
    "# print(param.Hardware.torch_multiprocessing_sharing_strategy)\n",
    "# print(param.Hardware.torch_threads)\n",
    "# print(param.HyperParameter.architecture)\n",
    "# print(device)\n",
    "# print(param.HyperParameter.emitter_label_photon_min)\n",
    "print(param.HyperParameter.optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_frame_ix_train = (0, 0)\n",
    "tar_frame_ix_test = (0, param.TestSet.test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 [-0.5, 39.5] [-0.5, 39.5] 0 0\n"
     ]
    }
   ],
   "source": [
    "# Turn to decode.neuralfitter.target_generator.ParameterListTarget \n",
    "n_max=param.HyperParameter.max_number_targets\n",
    "xextent=param.Simulation.psf_extent[0]\n",
    "yextent=param.Simulation.psf_extent[1]\n",
    "ix_low=tar_frame_ix_train[0]\n",
    "ix_high=tar_frame_ix_train[1]\n",
    "squeeze_batch_dim=True\n",
    "\n",
    "print(n_max,xextent,yextent,ix_low,ix_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31000.0 960.0 240.0\n"
     ]
    }
   ],
   "source": [
    "phot_max = param.Scaling.phot_max\n",
    "z_max = param.Scaling.z_max\n",
    "bg_max = param.Scaling.bg_max\n",
    "print(phot_max, z_max, bg_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator_train = sim_train\n",
    "simulator_test = sim_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model instantiated.\n",
      "Model initialised as specified in the constructor.\n"
     ]
    }
   ],
   "source": [
    "models_available = {\n",
    "    'SigmaMUNet': decode.neuralfitter.models.SigmaMUNet,\n",
    "    'DoubleMUnet': decode.neuralfitter.models.model_param.DoubleMUnet,\n",
    "    'SimpleSMLMNet': decode.neuralfitter.models.model_param.SimpleSMLMNet,\n",
    "}\n",
    "\n",
    "model = models_available[param.HyperParameter.architecture]\n",
    "model = model.parse(param)\n",
    "\n",
    "model_ls = decode.utils.model_io.LoadSaveModel(model, output_file=model_out)\n",
    "\n",
    "model = model_ls.load_init()\n",
    "model = model.to(torch.device(device))\n",
    "\n",
    "# Small collection of optimisers\n",
    "optimizer_available = {\n",
    "    'Adam': torch.optim.Adam,\n",
    "    'AdamW': torch.optim.AdamW\n",
    "}\n",
    "\n",
    "optimizer = optimizer_available[param.HyperParameter.optimizer]\n",
    "optimizer = optimizer(model.parameters(), **param.HyperParameter.opt_param)\n",
    "\n",
    "\"\"\"Loss function.\"\"\"\n",
    "criterion = decode.neuralfitter.loss.GaussianMMLoss(\n",
    "    xextent=param.Simulation.psf_extent[0],\n",
    "    yextent=param.Simulation.psf_extent[1],\n",
    "    img_shape=param.Simulation.img_size,\n",
    "    device=device,\n",
    "    chweight_stat=param.HyperParameter.chweight_stat)\n",
    "\n",
    "\"\"\"Learning Rate and Simulation Scheduling\"\"\"\n",
    "lr_scheduler_available = {\n",
    "    'ReduceLROnPlateau': torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    'StepLR': torch.optim.lr_scheduler.StepLR\n",
    "}\n",
    "lr_scheduler = lr_scheduler_available[param.HyperParameter.learning_rate_scheduler]\n",
    "lr_scheduler = lr_scheduler(optimizer, **param.HyperParameter.learning_rate_scheduler_param)\n",
    "\n",
    "\"\"\"Checkpointing\"\"\"\n",
    "checkpoint = CheckPoint(path=ckpt_path)\n",
    "\n",
    "\"\"\"Setup gradient modification\"\"\"\n",
    "grad_mod = param.HyperParameter.grad_mod\n",
    "\n",
    "\"\"\"Log the model (Graph) \"\"\"\n",
    "try:\n",
    "    dummy = torch.rand((2, param.HyperParameter.channels_in,\n",
    "                        *param.Simulation.img_size), requires_grad=False).to(\n",
    "        torch.device(device))\n",
    "    logger.add_graph(model, dummy)\n",
    "\n",
    "except:\n",
    "    print(\"Did not log graph.\")\n",
    "    # raise RuntimeError(\"Your dummy input is wrong. Please update it.\")\n",
    "\n",
    "\"\"\"Transform input data, compute weight mask and target data\"\"\"\n",
    "# frame_proc: x --> (x-offset)/scale\n",
    "frame_proc = decode.neuralfitter.scale_transform.AmplitudeRescale.parse(param)\n",
    "bg_frame_proc = None\n",
    "\n",
    "if param.HyperParameter.emitter_label_photon_min is not None:\n",
    "    # select emitters with photon > emitter_label_photon_min\n",
    "    em_filter = decode.neuralfitter.em_filter.PhotonFilter(\n",
    "        param.HyperParameter.emitter_label_photon_min)\n",
    "else:\n",
    "    em_filter = decode.neuralfitter.em_filter.NoEmitterFilter()\n",
    "\n",
    "tar_frame_ix_train = (0, 0)\n",
    "tar_frame_ix_test = (0, param.TestSet.test_size)\n",
    "\n",
    "\"\"\"Setup Target generator consisting possibly multiple steps in a transformation sequence.\"\"\"\n",
    "tar_gen = decode.neuralfitter.utils.processing.TransformSequence(\n",
    "    [\n",
    "        # emitter_set --> Tuple (param_tar, mask_tar, bg)\n",
    "        # param_tar: N*4, [photon, xyz], mask_tar: N*4, 0/1, indicate function \n",
    "        decode.neuralfitter.target_generator.ParameterListTarget(\n",
    "            n_max=param.HyperParameter.max_number_targets,\n",
    "            xextent=param.Simulation.psf_extent[0],\n",
    "            yextent=param.Simulation.psf_extent[1],\n",
    "            ix_low=tar_frame_ix_train[0],\n",
    "            ix_high=tar_frame_ix_train[1],\n",
    "            squeeze_batch_dim=True),\n",
    "\n",
    "        decode.neuralfitter.target_generator.DisableAttributes.parse(param),\n",
    "\n",
    "        # param_tar --> phot/max, z/z_max, bg/bg_max\n",
    "        decode.neuralfitter.scale_transform.ParameterListRescale(\n",
    "            phot_max=param.Scaling.phot_max,\n",
    "            z_max=param.Scaling.z_max,\n",
    "            bg_max=param.Scaling.bg_max)\n",
    "    ])\n",
    "\n",
    "# setup target for test set in similar fashion, however test-set is static.\n",
    "tar_gen_test = copy.deepcopy(tar_gen)\n",
    "tar_gen_test.com[0].ix_low = tar_frame_ix_test[0]\n",
    "tar_gen_test.com[0].ix_high = tar_frame_ix_test[1]\n",
    "tar_gen_test.com[0].squeeze_batch_dim = False\n",
    "tar_gen_test.com[0].sanity_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_dataloader(param, train_ds, test_ds=None):\n",
    "    \"\"\"Set up dataloader\"\"\"\n",
    "\n",
    "    train_dl = torch.utils.data.DataLoader(\n",
    "        dataset=train_ds,\n",
    "        batch_size=param.HyperParameter.batch_size,\n",
    "        drop_last=True,\n",
    "        shuffle=True,\n",
    "        num_workers=param.Hardware.num_worker_train,\n",
    "        pin_memory=True,\n",
    "        collate_fn=decode.neuralfitter.utils.dataloader_customs.smlm_collate)\n",
    "\n",
    "    if test_ds is not None:\n",
    "\n",
    "        test_dl = torch.utils.data.DataLoader(\n",
    "            dataset=test_ds,\n",
    "            batch_size=param.HyperParameter.batch_size,\n",
    "            drop_last=False,\n",
    "            shuffle=False,\n",
    "            num_workers=param.Hardware.num_worker_train,\n",
    "            pin_memory=False,\n",
    "            collate_fn=decode.neuralfitter.utils.dataloader_customs.smlm_collate)\n",
    "    else:\n",
    "\n",
    "        test_dl = None\n",
    "\n",
    "    return train_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled dataset in 21.48s. 250965 emitters on 10001 frames.\n"
     ]
    }
   ],
   "source": [
    "train_ds = decode.neuralfitter.dataset.SMLMLiveDataset(\n",
    "    simulator=simulator_train,\n",
    "    em_proc=em_filter,\n",
    "    frame_proc=frame_proc,\n",
    "    bg_frame_proc=bg_frame_proc,\n",
    "    tar_gen=tar_gen, weight_gen=None,\n",
    "    frame_window=param.HyperParameter.channels_in,\n",
    "    pad=None, return_em=False)\n",
    "\n",
    "train_ds.sample(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled dataset in 1.15s. 12438 emitters on 513 frames.\n"
     ]
    }
   ],
   "source": [
    "test_ds = decode.neuralfitter.dataset.SMLMAPrioriDataset(\n",
    "    simulator=simulator_test,\n",
    "    em_proc=em_filter,\n",
    "    frame_proc=frame_proc,\n",
    "    bg_frame_proc=bg_frame_proc,\n",
    "    tar_gen=tar_gen_test, weight_gen=None,\n",
    "    frame_window=param.HyperParameter.channels_in,\n",
    "    pad=None, return_em=False)\n",
    "\n",
    "test_ds.sample(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = train_ds\n",
    "ds_test = test_ds\n",
    "dl_train, dl_test = setup_dataloader(param, ds_train, ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/156 [19:29<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "dataloader = dl_train\n",
    "tqdm_enum = tqdm(dataloader, total=len(dataloader), smoothing=0.)  # progress bar enumeration\n",
    "ttt = iter(tqdm_enum)\n",
    "(x, y_tar, weight) = next(ttt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 40, 40])\n",
      "torch.Size([64, 250, 4])\n",
      "torch.Size([64, 250])\n",
      "torch.Size([64, 40, 40])\n"
     ]
    }
   ],
   "source": [
    "print(x.size())\n",
    "print(y_tar[0].size())\n",
    "print(y_tar[1].size())\n",
    "print(y_tar[2].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Loss function.\"\"\"\n",
    "criterion = decode.neuralfitter.loss.GaussianMMLoss(\n",
    "    xextent=param.Simulation.psf_extent[0],\n",
    "    yextent=param.Simulation.psf_extent[1],\n",
    "    img_shape=param.Simulation.img_size,\n",
    "    device=device,\n",
    "    chweight_stat=param.HyperParameter.chweight_stat)\n",
    "\n",
    "loss = criterion\n",
    "\n",
    "\"\"\"Forward the data\"\"\"\n",
    "y_out = model(x)\n",
    "\n",
    "\"\"\"Reset the optimiser, compute the loss and backprop it\"\"\"\n",
    "loss_val = loss(y_out, y_tar, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss_val.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "label_path = '/media/hdd/rPSF_data/rPSF/train/0620_uniformFlux/label.txt'\n",
    "label_raw = np.loadtxt(label_path)\n",
    "if label_raw.ndim < 2:\n",
    "    label_raw = np.expand_dims(label_raw, axis=0)\n",
    "labels = {}\n",
    "for i in np.unique(label_raw[:,0]):\n",
    "    i_bol = label_raw[:,0] == i\n",
    "    labels[i] = label_raw[i_bol,:]\n",
    "    if labels[i].ndim < 2:\n",
    "        labels[i] = torch.tensor(labels[i]).unsqueeze(0)\n",
    "    else:\n",
    "        labels[i] = torch.tensor(labels[i])\n",
    "    labels[i] =  labels[i][:,[4,1,2,3]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[113.5610,  20.4191,  27.5939,  -4.3109],\n",
      "        [105.1080, -24.3517, -25.3649,   6.2191],\n",
      "        [139.0421,  -5.3202,  28.1096, -13.1525],\n",
      "        [121.4389,  28.2700,   9.0004,   8.2418],\n",
      "        [119.1001,  19.8701, -27.3673, -18.7267],\n",
      "        [144.9537,  31.2455, -15.0621,  -8.9231],\n",
      "        [100.4356,  10.5904,   3.1879, -18.1531],\n",
      "        [133.7012, -31.5716,  31.1105, -16.1147],\n",
      "        [133.6259,  23.7408,  31.6124,  12.9383],\n",
      "        [106.9572,  29.5115, -23.2823,   7.7931],\n",
      "        [120.3302,  12.1540,  32.0003,  -7.3160],\n",
      "        [ 85.5178,  17.5263,  31.0874,  18.0089],\n",
      "        [ 83.8938,  16.5330,  -0.9945, -18.6222],\n",
      "        [117.7493,  19.5232,  28.6126,  -0.7169],\n",
      "        [135.3023,  22.2580,  27.5873,  -7.5032],\n",
      "        [146.3889,  20.4429,  26.6142,  -0.3294],\n",
      "        [ 89.2350,  22.1361,  26.5680,  -7.3112],\n",
      "        [120.3523,  19.4234,  26.3802,  -1.9900]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(labels[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in labels.keys():\n",
    "    if len(labels[i].shape)==2:\n",
    "        labels[i] = np.expand_dims(labels[i].T,0) # [1,n,3]\n",
    "    elif len(labels[i].shape)==1: # labels[i] = 3*n, n is number of source points,\n",
    "        labels[i] = np.expand_dims(labels[i].T,0) # [1,3]\n",
    "        labels[i] = np.expand_dims(labels[i],0) # [1,1,3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.000e+00, 2.000e+00, 3.000e+00, ..., 9.998e+03, 9.999e+03,\n",
       "       1.000e+04])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(label_raw[:,0])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "32d9234fb3a060fbd30877034f28c0fca724fa3d0d25a605ad46b1806c555f07"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('decode_dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
