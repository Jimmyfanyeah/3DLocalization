{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# a = torch.randn((2,41,96,96))\n",
    "# b = torch.randn((2,250,96,96))\n",
    "# #\n",
    "# c = torch.square(a-b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lingjia/anaconda3/envs/deepstorm3d/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# official modules\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from math import floor\n",
    "from shutil import copy2\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "# self-defined module\n",
    "from utils.helper import init_DDP, Logger, print_log, load_labels, build_model\n",
    "from utils.data import dataloader\n",
    "from utils.train_model import train_model\n",
    "from utils.test_model import test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    parser = argparse.ArgumentParser(description='3d localization')\n",
    "    # phase\n",
    "    parser.add_argument('--train_or_test', type=str, default='other', help='train or test')\n",
    "    parser.add_argument('--resume', action='store_true', default=False)\n",
    "    parser.add_argument('--gpu_number', type=str, default=None, help='assign gpu')\n",
    "    # data info\n",
    "    parser.add_argument('--num_im', type=int, default=None, help='Number of samples used, train:val=9:1')\n",
    "    parser.add_argument('--H', type=int, default=96, help='Height of image')\n",
    "    parser.add_argument('--W', type=int, default=96, help='Width of image')\n",
    "    parser.add_argument('--zmin', type=int, default=-20, help='min zeta')\n",
    "    parser.add_argument('--zmax', type=int, default=20, help='max zeta')\n",
    "    parser.add_argument('--clear_dist', type=int, default=1, help='safe margin for z axis')\n",
    "    parser.add_argument('--D', type=int, default=250, help='num grid of zeta axis')\n",
    "    parser.add_argument('--scaling_factor', type=int, default=800, help='entry value for existence of pts')\n",
    "    parser.add_argument('--upsampling_factor', type=int, default=2, help='grid dim=[H,W]*upsampling_factor')\n",
    "    # train info\n",
    "    parser.add_argument('--model_use', type=str, default='LocNet')\n",
    "    parser.add_argument('--postpro',  action='store_true', default=False, help='whether do post processing in dnn')\n",
    "    parser.add_argument('--batch_size', type=int, default=1, help='when training on multi GPU, is the batch size on each GPU')\n",
    "    parser.add_argument('--initial_learning_rate', type=float, default=None, help='initial learning rate for adam')\n",
    "    parser.add_argument('--lr_decay_per_epoch', type=int, default=None, help='number of epochs learning rate decay')\n",
    "    parser.add_argument('--lr_decay_factor', type=float, default=None, help='lr decay factor')\n",
    "    parser.add_argument('--max_epoch', type=int,   default=None, help='number of training epoches')\n",
    "    parser.add_argument('--save_epoch', type=int, default=None, help='save model per save_epoch')\n",
    "    # test info\n",
    "    parser.add_argument('--test_id_loc', type=str, default=None)\n",
    "    # path\n",
    "    parser.add_argument('--checkpoint_path', type=str,  default=None,  help='checkpoint to resume from')\n",
    "    parser.add_argument('--data_path', type=str, default='/home/lingjia/Documents/microscope/Data/training_images_zrange20', help='path for train and val data')\n",
    "    parser.add_argument('--save_path', type=str, default=None, help='path for save models and results')\n",
    "    # output\n",
    "    parser.add_argument('--name_time', type=str, default=None, help='string of running time')\n",
    "    # for nonconvex loss\n",
    "    parser.add_argument('--port', type=str, default=None, help='DDP master port')\n",
    "    parser.add_argument('--weight', type=str, default=None, help='lambda CEL0')\n",
    "    parser.add_argument('--extra_loss', type=str, default=None, help='indicate whether use cel0 for gaussian or nc for possion')\n",
    "    # for extra losses\n",
    "    parser.add_argument('--cel0_mu', type=float, default=None, help='mu in cel0 loss')\n",
    "    parser.add_argument('--klnc_a', type=float, default=None, help='a for nonconvex loss in KLNC')\n",
    "    parser.add_argument('--log_comment', type=str, default=None)\n",
    "    \n",
    "    # opt = parser.parse_args()\n",
    "    opt,_=parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.gpu_number='1'\n",
    "opt.train_or_test='train'\n",
    "opt.name_time=\"2022-99-99-99-99-99\"\n",
    "opt.num_im=10000\n",
    "opt.H=96\n",
    "opt.W=96\n",
    "opt.zmin=-20\n",
    "opt.zmax=20\n",
    "opt.clear_dist=1\n",
    "opt.D=250\n",
    "opt.scaling_factor=800\n",
    "opt.upsampling_factor=2\n",
    "opt.model_use='LocNet'\n",
    "opt.batch_size=16\n",
    "opt.initial_learning_rate=1e-3\n",
    "opt.lr_decay_per_epoch=7\n",
    "opt.lr_decay_factor=0.5\n",
    "opt.max_epoch=2\n",
    "opt.save_epoch=10\n",
    "opt.data_path='/media/hdd/lingjia/hdd_rpsf/20220917_nonconvex_loss/data/gaussian_10k_pt50L5'\n",
    "opt.save_path='./temp'\n",
    "opt.port='123789'\n",
    "opt.weight='1_1_1_1'\n",
    "opt.extra_loss='mse3d_cel0_klnc_forward'\n",
    "opt.cel0_mu=1\n",
    "opt.klnc_a=10\n",
    "opt.rank = 0\n",
    "opt.world_size = 1\n",
    "\n",
    "rank = opt.rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO==>] setup_params:\n",
      "D: 250\n",
      "H: 96\n",
      "W: 96\n",
      "batch_size: 16\n",
      "cel0_mu: 1\n",
      "checkpoint_path: None\n",
      "clear_dist: 1\n",
      "data_path: /media/hdd/lingjia/hdd_rpsf/20220917_nonconvex_loss/data/gaussian_10k_pt50L5\n",
      "extra_loss: mse3d_cel0_klnc_forward\n",
      "gpu_number: 1\n",
      "initial_learning_rate: 0.001\n",
      "klnc_a: 10\n",
      "log_comment: None\n",
      "lr_decay_factor: 0.5\n",
      "lr_decay_per_epoch: 7\n",
      "max_epoch: 2\n",
      "model_use: LocNet\n",
      "name_time: 2022-99-99-99-99-99\n",
      "ntrain: 9000\n",
      "num_im: 10000\n",
      "nval: 1000\n",
      "pixel_size_axial: 0.172\n",
      "port: 123789\n",
      "postpro: False\n",
      "rank: 0\n",
      "resume: False\n",
      "save_epoch: 10\n",
      "save_path: ./temp/2022-99-99-99-99-99-lr0.001-bs16-D250-Ep2-nT9000-w1_1_1_1-mse3d_cel0_klnc_forward-1-10-LocNet\n",
      "scaling_factor: 800\n",
      "test_id_loc: None\n",
      "train_or_test: train\n",
      "upsampling_factor: 2\n",
      "weight: 1_1_1_1\n",
      "world_size: 1\n",
      "zmax: 20\n",
      "zmin: -20\n",
      "[INFO==>] Dataset: Train 9000 Val 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResLocalizationCNN(\n",
       "  (norm): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Conv2DLeakyReLUBN(\n",
       "    (conv): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (layer2): ResConv2DLeakyReLUBN(\n",
       "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (layer3): ResConv2DLeakyReLUBN(\n",
       "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (layer4): ResConv2DLeakyReLUBN(\n",
       "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4))\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (layer5): ResConv2DLeakyReLUBN(\n",
       "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(8, 8), dilation=(8, 8))\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (layer6): ResConv2DLeakyReLUBN(\n",
       "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(16, 16), dilation=(16, 16))\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (deconv1): ResConv2DLeakyReLUBN(\n",
       "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (deconv2): ResConv2DLeakyReLUBN(\n",
       "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (layer7): Conv2DLeakyReLUBN(\n",
       "    (conv): Conv2d(64, 250, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (bn): BatchNorm2d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (layer8): ResConv2DLeakyReLUBN(\n",
       "    (conv): Conv2d(250, 250, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (bn): BatchNorm2d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (layer9): ResConv2DLeakyReLUBN(\n",
       "    (conv): Conv2d(250, 250, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (bn): BatchNorm2d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (layer10): Conv2d(250, 250, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (pred): Hardtanh(min_val=0.0, max_val=800)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate zoom ratio of z-axis\n",
    "opt.pixel_size_axial = (opt.zmax - opt.zmin + 1 + 2*opt.clear_dist) / opt.D\n",
    "\n",
    "# split dataset to train, validation 9:1\n",
    "train_IDs = np.arange(1,floor(opt.num_im*0.9)+1,1).tolist()\n",
    "val_IDs = np.arange(floor(opt.num_im*0.9)+1,opt.num_im+1).tolist()\n",
    "\n",
    "opt.partition = {'train': train_IDs, 'valid': val_IDs}\n",
    "opt.ntrain, opt.nval = len(train_IDs), len(val_IDs)\n",
    "\n",
    "# output folder name\n",
    "name_time = opt.name_time if opt.name_time else time.strftime('%Y-%m-%d-%H-%M-%S')\n",
    "save_name = name_time + '-lr'+str(opt.initial_learning_rate) + \\\n",
    "    '-bs'+str(opt.batch_size) + \\\n",
    "    '-D'+str(opt.D) + \\\n",
    "    '-Ep'+str(opt.max_epoch) + \\\n",
    "    '-nT'+str(opt.ntrain)\n",
    "if opt.extra_loss:\n",
    "    save_name = save_name + '-w' + str(opt.weight) + '-' + str(opt.extra_loss)\n",
    "if opt.cel0_mu:\n",
    "    save_name = save_name + '-' + str(opt.cel0_mu)\n",
    "if opt.klnc_a:\n",
    "    save_name = save_name + '-' + str(opt.klnc_a)\n",
    "save_name = save_name + '-' + str(opt.model_use)\n",
    "\n",
    "if opt.resume:\n",
    "    save_name = save_name + '-resume'\n",
    "if opt.postpro:\n",
    "    save_name = save_name + '-postpro'\n",
    "opt.save_path = os.path.join(opt.save_path,save_name)\n",
    "os.makedirs(opt.save_path, exist_ok=True)\n",
    "\n",
    "if rank == 0:\n",
    "    log = open(os.path.join(opt.save_path, '{}_log.txt'.format(time.strftime('%H-%M-%S'))), 'w')\n",
    "    logger = Logger(os.path.join(opt.save_path, '{}_tensorboard'.format(time.strftime('%H-%M-%S'))))\n",
    "\n",
    "    print_log('[INFO==>] setup_params:',log)\n",
    "    for key,value in opt._get_kwargs():\n",
    "        if not key == 'partition':\n",
    "            print_log('{}: {}'.format(key,value),log)\n",
    "    print_log(f'[INFO==>] Dataset: Train {len(train_IDs)} Val {len(val_IDs)}',log)\n",
    "\n",
    "device = torch.device('cuda')\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "if opt.rank==0:\n",
    "    # save setup parameters in result folder as well\n",
    "    with open(os.path.join(opt.save_path,'setup_params.json'),'w') as handle:\n",
    "        json.dump(opt.__dict__, handle, indent=2)\n",
    "\n",
    "# Load labels and generate dataset\n",
    "labels = load_labels(os.path.join(opt.data_path,'label.txt'))\n",
    "\n",
    "# Parameters for dataloaders\n",
    "params_train = {'batch_size': opt.batch_size, 'shuffle': True,  'partition': opt.partition['train']}\n",
    "params_val = {'batch_size': opt.batch_size, 'shuffle': False, 'partition': opt.partition['valid']}\n",
    "\n",
    "training_generator = dataloader(opt.data_path, labels, params_train, opt, num_workers=0)\n",
    "validation_generator = dataloader(opt.data_path, labels, params_val, opt, num_workers=0)\n",
    "\n",
    "# model\n",
    "model = build_model(opt)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO==>] Number of parameters: 1594162\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = Adam(list(model.parameters()), lr=opt.initial_learning_rate)\n",
    "\n",
    "# opt.scheduler_type = 'ReduceLROnPlateau'\n",
    "# if opt.scheduler_type == 'StepLR':\n",
    "# scheduler = lr_scheduler.StepLR(optimizer, step_size=opt.lr_decay_per_epoch, gamma=opt.lr_decay_factor)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=opt.lr_decay_factor, patience=opt.lr_decay_per_epoch)\n",
    "\n",
    "if opt.rank == 0:\n",
    "    # print_log(model, log)\n",
    "    print_log(\"[INFO==>] Number of parameters: {}\".format(sum(param.numel() for param in model.parameters())),log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model(model,optimizer,scheduler,device,training_generator,validation_generator,log,logger,opt)\n",
    "# official modules\n",
    "import os\n",
    "import time\n",
    "from time import localtime, strftime\n",
    "from math import ceil\n",
    "import pickle5 as pickle\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.distributed as dist\n",
    "# self-defined modules\n",
    "from utils.loss import calculate_loss\n",
    "from utils.helper import print_log, print_metrics, print_time, print_metric_format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO==>] Loss types: ['loss', 'mse3d', 'cel0', 'klnc', 'forward']\n",
      "(6, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "learning_results = defaultdict(list)\n",
    "max_epoch = opt.max_epoch\n",
    "\n",
    "steps_train = ceil(opt.ntrain / opt.batch_size / opt.world_size)\n",
    "steps_val = ceil(opt.nval / opt.batch_size / opt.world_size)\n",
    "params_val = {'batch_size': opt.batch_size, 'shuffle': False}\n",
    "\n",
    "# loss function\n",
    "loss_type = ['loss']\n",
    "extra_weight = []\n",
    "if opt.extra_loss: # None or string\n",
    "    extra_loss = opt.extra_loss.split('_')\n",
    "    extra_weight = [float(n) for n in opt.weight.split('_')]\n",
    "    loss_type = loss_type + extra_loss\n",
    "    if not len(extra_loss) == len(extra_weight):\n",
    "        raise Exception(f'Input {len(extra_loss)} weight with {len(extra_weight)} extra loss')\n",
    "print_log(f'[INFO==>] Loss types: {loss_type}',log)\n",
    "calc_loss = calculate_loss(opt,loss_type,extra_weight)\n",
    "\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO==>] Start training from 0 to 2 rank 0\n",
      "\n",
      "---> Epoch 1/2 | 2022-12-24 15:25:18 | lr 0.001\n"
     ]
    }
   ],
   "source": [
    "# start from scratch\n",
    "start_epoch, end_epoch = 0, max_epoch\n",
    "learning_results = {'val_max': [], 'val_sum': [], 'steps_per_epoch': steps_train}\n",
    "for loss in loss_type:\n",
    "    learning_results['train_'+loss] = []\n",
    "    learning_results['val_'+loss] = []\n",
    "best_val_loss = float('Inf')\n",
    "\n",
    "# starting time of training\n",
    "train_start_time = time.time()\n",
    "not_improve = 0\n",
    "\n",
    "print_log(f'[INFO==>] Start training from {start_epoch} to {end_epoch} rank {opt.rank}\\n',log)\n",
    "\n",
    "epoch = 0\n",
    "# starting time of current epoch\n",
    "epoch_start_time = time.time()\n",
    "\n",
    "if opt.rank == 0:\n",
    "    print_log(f'Epoch {epoch+1}/{end_epoch} | {strftime(\"%Y-%m-%d %H:%M:%S\", localtime())} | lr {optimizer.param_groups[0][\"lr\"]}', log, arrow=True)\n",
    "\n",
    "# training phase\n",
    "model.train()\n",
    "metric, metrics = defaultdict(float), defaultdict(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 96, 96]) \n",
      "torch.Size([16, 250, 192, 192]) \n",
      "torch.Size([16, 96, 96]) \n",
      "['45', '8188', '4690', '5863', '7076', '1660', '7640', '8977', '7346', '559', '4862', '5116', '5005', '3935', '6155', '6027']\n",
      "Output shape:\n",
      "torch.Size([16, 250, 192, 192])\n",
      "upgrid:\n",
      "torch.Size([16, 250, 192, 192])\n",
      "spikes_pred:\n",
      "torch.Size([16, 250, 96, 96])\n",
      "pool info: (2, 2)\n",
      "spikes_pred:\n",
      "torch.Size([16, 250, 96, 96])\n",
      "norm_ai2:\n",
      "torch.Size([250, 96, 96])\n",
      "abs_heat:\n",
      "torch.Size([16, 250, 96, 96])\n",
      "Epoch 0/1 Train 0/562 MaxOut 31.69 mse3d 55.7752  cel0 nan  klnc 4513019.0000  forward 752305984.0000  loss nan  \n"
     ]
    }
   ],
   "source": [
    "with torch.set_grad_enabled(True):\n",
    "    batch_ind = 0\n",
    "    (inputs, targets, target_ims, fileids) = next(iter(training_generator))\n",
    "    print(f\"{inputs.shape} \\n{targets.shape} \\n{target_ims.shape} \\n{fileids}\")\n",
    "    # for batch_ind, (inputs, targets, target_ims, fileids) in enumerate(training_generator):\n",
    "\n",
    "    inputs = inputs.to(device)\n",
    "    targets = targets.to(device)\n",
    "    target_ims = target_ims.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    with autocast():\n",
    "        outputs = model(inputs)\n",
    "    print(f\"Output shape:\\n{outputs.shape}\")\n",
    "    loss = calc_loss(upgrid=outputs.float(), gt_upgrid=targets, gt_im=target_ims, metric=metric, metrics=metrics)\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    if opt.rank==0:\n",
    "        print(f'Epoch {epoch}/{end_epoch-1} Train {batch_ind}/{steps_train-1} MaxOut {outputs.max():.2f} {print_metric_format(metric)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.nan\n",
    "print(a)\n",
    "print(a*0)\n",
    "print(18*1+a*0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepstorm3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c40f20e5cf12c01a8510f5b704b157892e0eb193fc1e0d9c4d1938b744ef0863"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
